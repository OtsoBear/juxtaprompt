{"version":3,"file":"llm-services-bfcb06ee.js","sources":["../../src/schemas/llm-schemas.ts","../../src/services/llm/base-llm-provider.ts","../../src/services/llm/llm-provider-manager.ts","../../src/services/llm/providers/openai-provider.ts","../../src/services/llm/providers/anthropic-provider.ts","../../src/services/llm/providers/gemini-provider.ts","../../src/services/llm/index.ts"],"sourcesContent":["// src/schemas/llm-schemas.ts\nimport { z } from 'zod';\nimport type { ValidationResult } from '@/types/llm';\n\n// OpenAI Response Schema\nexport const OpenAIStreamChunkSchema = z.object({\n  id: z.string(),\n  object: z.literal('chat.completion.chunk'),\n  created: z.number(),\n  model: z.string(),\n  choices: z.array(z.object({\n    index: z.number(),\n    delta: z.object({\n      content: z.string().optional(),\n      role: z.string().optional(),\n    }),\n    finish_reason: z.string().nullable(),\n  })),\n});\n\nexport const OpenAIErrorSchema = z.object({\n  error: z.object({\n    message: z.string(),\n    type: z.string(),\n    param: z.string().nullable().optional(),\n    code: z.string().nullable().optional(),\n  }),\n});\n\n// Anthropic Response Schema\nexport const AnthropicStreamChunkSchema = z.object({\n  type: z.enum(['message_start', 'content_block_start', 'content_block_delta', 'content_block_stop', 'message_delta', 'message_stop']),\n  message: z.object({\n    id: z.string(),\n    type: z.literal('message'),\n    role: z.literal('assistant'),\n    content: z.array(z.object({\n      type: z.literal('text'),\n      text: z.string(),\n    })),\n    model: z.string(),\n    stop_reason: z.string().nullable().optional(),\n    stop_sequence: z.string().nullable().optional(),\n    usage: z.object({\n      input_tokens: z.number(),\n      output_tokens: z.number(),\n    }).optional(),\n  }).optional(),\n  delta: z.object({\n    type: z.literal('text_delta').optional(),\n    text: z.string().optional(),\n    stop_reason: z.string().nullable().optional(),\n    stop_sequence: z.string().nullable().optional(),\n  }).optional(),\n  content_block: z.object({\n    type: z.literal('text'),\n    text: z.string(),\n  }).optional(),\n  index: z.number().optional(),\n});\n\nexport const AnthropicErrorSchema = z.object({\n  type: z.literal('error'),\n  error: z.object({\n    type: z.string(),\n    message: z.string(),\n  }),\n});\n\n// Gemini Response Schema\nexport const GeminiStreamChunkSchema = z.object({\n  candidates: z.array(z.object({\n    content: z.object({\n      parts: z.array(z.object({\n        text: z.string(),\n      })),\n      role: z.string(),\n    }),\n    finishReason: z.string().optional(),\n    index: z.number(),\n    safetyRatings: z.array(z.object({\n      category: z.string(),\n      probability: z.string(),\n    })).optional(),\n  })),\n  promptFeedback: z.object({\n    safetyRatings: z.array(z.object({\n      category: z.string(),\n      probability: z.string(),\n    })),\n  }).optional(),\n});\n\nexport const GeminiErrorSchema = z.object({\n  error: z.object({\n    code: z.number(),\n    message: z.string(),\n    status: z.string(),\n  }),\n});\n\n// LLM Configuration Schema\nexport const LLMConfigSchema = z.object({\n  provider: z.enum(['openai', 'anthropic', 'gemini']),\n  apiKey: z.string().min(1, 'API key is required'),\n  baseUrl: z.string().url('Invalid base URL'),\n  model: z.string().min(1, 'Model is required'),\n  temperature: z.number().min(0).max(2),\n  maxTokens: z.number().min(1).max(32000),\n  topP: z.number().min(0).max(1),\n  frequencyPenalty: z.number().min(-2).max(2),\n  presencePenalty: z.number().min(-2).max(2),\n  systemMessage: z.string(),\n});\n\n// URL State Schema\nexport const URLStateSchema = z.object({\n  prompts: z.array(z.string()),\n  config: z.object({\n    provider: z.enum(['openai', 'anthropic', 'gemini']).optional(),\n    model: z.string().optional(),\n    temperature: z.number().min(0).max(2).optional(),\n    maxTokens: z.number().min(1).max(32000).optional(),\n    topP: z.number().min(0).max(1).optional(),\n    frequencyPenalty: z.number().min(-2).max(2).optional(),\n    presencePenalty: z.number().min(-2).max(2).optional(),\n    systemMessage: z.string().optional(),\n  }),\n  ui: z.object({\n    gridColumns: z.number().min(1).max(6).optional(),\n    autoSend: z.boolean().optional(),\n    debounceMs: z.number().min(0).max(5000).optional(),\n    showAdvancedSettings: z.boolean().optional(),\n    theme: z.enum(['light', 'dark', 'system']).optional(),\n  }),\n});\n\n// Storage Preference Schema\nexport const StoragePreferenceSchema = z.object({\n  type: z.enum(['none', 'session', 'local']),\n  acknowledgedRisks: z.boolean(),\n});\n\n// Validation helper functions\nexport function validateOpenAIResponse(data: unknown): ValidationResult<z.infer<typeof OpenAIStreamChunkSchema>> {\n  const result = OpenAIStreamChunkSchema.safeParse(data);\n  if (result.success) {\n    return { success: true, data: result.data };\n  }\n  \n  return {\n    success: false,\n    error: {\n      code: 'VALIDATION_ERROR',\n      message: 'Invalid OpenAI response format',\n      details: result.error.issues,\n    },\n  };\n}\n\nexport function validateAnthropicResponse(data: unknown): ValidationResult<z.infer<typeof AnthropicStreamChunkSchema>> {\n  const result = AnthropicStreamChunkSchema.safeParse(data);\n  if (result.success) {\n    return { success: true, data: result.data };\n  }\n  \n  return {\n    success: false,\n    error: {\n      code: 'VALIDATION_ERROR',\n      message: 'Invalid Anthropic response format',\n      details: result.error.issues,\n    },\n  };\n}\n\nexport function validateGeminiResponse(data: unknown): ValidationResult<z.infer<typeof GeminiStreamChunkSchema>> {\n  const result = GeminiStreamChunkSchema.safeParse(data);\n  if (result.success) {\n    return { success: true, data: result.data };\n  }\n  \n  return {\n    success: false,\n    error: {\n      code: 'VALIDATION_ERROR',\n      message: 'Invalid Gemini response format',\n      details: result.error.issues,\n    },\n  };\n}\n\nexport function validateLLMConfig(data: unknown): ValidationResult<z.infer<typeof LLMConfigSchema>> {\n  const result = LLMConfigSchema.safeParse(data);\n  if (result.success) {\n    return { success: true, data: result.data };\n  }\n  \n  return {\n    success: false,\n    error: {\n      code: 'VALIDATION_ERROR',\n      message: 'Invalid LLM configuration',\n      details: result.error.issues,\n    },\n  };\n}\n\nexport function validateURLState(data: unknown): ValidationResult<z.infer<typeof URLStateSchema>> {\n  const result = URLStateSchema.safeParse(data);\n  if (result.success) {\n    return { success: true, data: result.data };\n  }\n  \n  return {\n    success: false,\n    error: {\n      code: 'VALIDATION_ERROR',\n      message: 'Invalid URL state format',\n      details: result.error.issues,\n    },\n  };\n}\n\nexport function validateStoragePreference(data: unknown): ValidationResult<z.infer<typeof StoragePreferenceSchema>> {\n  const result = StoragePreferenceSchema.safeParse(data);\n  if (result.success) {\n    return { success: true, data: result.data };\n  }\n  \n  return {\n    success: false,\n    error: {\n      code: 'VALIDATION_ERROR',\n      message: 'Invalid storage preference',\n      details: result.error.issues,\n    },\n  };\n}\n\n// Export schema types\nexport type OpenAIStreamChunk = z.infer<typeof OpenAIStreamChunkSchema>;\nexport type AnthropicStreamChunk = z.infer<typeof AnthropicStreamChunkSchema>;\nexport type GeminiStreamChunk = z.infer<typeof GeminiStreamChunkSchema>;\nexport type ValidatedLLMConfig = z.infer<typeof LLMConfigSchema>;\nexport type ValidatedURLState = z.infer<typeof URLStateSchema>;\nexport type ValidatedStoragePreference = z.infer<typeof StoragePreferenceSchema>;","// src/services/llm/base-llm-provider.ts\nimport type {\n  ILLMProvider,\n  LLMRequest,\n  LLMStreamChunk,\n  LLMConfig,\n  LLMError,\n  ValidationResult,\n  LLMProvider,\n  ModelInfo,\n  AvailableModelsResult\n} from '@/types/llm';\nimport { validateLLMConfig } from '@/schemas/llm-schemas';\n\n/**\n * Custom error class for LLM-related errors\n */\nexport class LLMProviderError extends Error {\n  public readonly code: string;\n  public readonly retryable: boolean;\n  public readonly statusCode?: number;\n  public readonly details?: unknown;\n\n  constructor(error: LLMError) {\n    super(error.message);\n    this.name = 'LLMProviderError';\n    this.code = error.code;\n    this.retryable = error.retryable;\n    if (error.statusCode !== undefined) {\n      this.statusCode = error.statusCode;\n    }\n    if (error.details !== undefined) {\n      this.details = error.details;\n    }\n  }\n}\n\n/**\n * Abstract base class for LLM providers with common functionality\n */\nexport abstract class BaseLLMProvider implements ILLMProvider {\n  public abstract readonly name: LLMProvider;\n  \n  protected readonly defaultTimeout = 30000; // 30 seconds\n  protected readonly maxRetries = 3;\n  \n  // Optimized model caching with WeakMap for better memory management\n  private static modelCache = new Map<string, { models: ModelInfo[]; timestamp: number }>();\n  private readonly cacheTimeout = 10 * 60 * 1000; // 10 minutes (longer cache)\n\n  /**\n   * Send streaming request to the LLM provider\n   */\n  public abstract sendStreamingRequest(request: LLMRequest): AsyncIterable<LLMStreamChunk>;\n\n  /**\n   * Get available models from the provider API\n   */\n  public async getAvailableModels(apiKey: string, baseUrl?: string): Promise<AvailableModelsResult> {\n    const cacheKey = `${this.name}_${apiKey.slice(-8)}_${baseUrl || 'default'}`;\n    const cached = BaseLLMProvider.modelCache.get(cacheKey);\n    \n    // Return cached result if still valid\n    if (cached && Date.now() - cached.timestamp < this.cacheTimeout) {\n      return {\n        models: cached.models,\n        cached: true,\n        timestamp: cached.timestamp,\n      };\n    }\n\n    try {\n      const models = await this.fetchAvailableModels(apiKey, baseUrl);\n      \n      // Cache the result\n      BaseLLMProvider.modelCache.set(cacheKey, {\n        models,\n        timestamp: Date.now(),\n      });\n\n      return {\n        models,\n        cached: false,\n        timestamp: Date.now(),\n      };\n    } catch (error) {\n      // If API call fails, return cached result if available, otherwise fallback to static models\n      if (cached) {\n        console.warn(`Failed to fetch models for ${this.name}, using cached result:`, error);\n        return {\n          models: cached.models,\n          cached: true,\n          timestamp: cached.timestamp,\n        };\n      }\n      \n      // Fallback to static models\n      console.warn(`Failed to fetch models for ${this.name}, using fallback:`, error);\n      return {\n        models: this.getFallbackModels(),\n        cached: false,\n        timestamp: Date.now(),\n      };\n    }\n  }\n\n  /**\n   * Fetch available models from the provider API (to be implemented by subclasses)\n   */\n  protected abstract fetchAvailableModels(apiKey: string, baseUrl?: string): Promise<ModelInfo[]>;\n\n  /**\n   * Get fallback models when API call fails (to be implemented by subclasses)\n   */\n  protected abstract getFallbackModels(): ModelInfo[];\n\n  /**\n   * Clear model cache\n   */\n  public clearModelCache(): void {\n    BaseLLMProvider.modelCache.clear();\n  }\n\n  /**\n   * Validate LLM configuration\n   */\n  public validateConfig(config: Partial<LLMConfig>): ValidationResult<LLMConfig> {\n    return validateLLMConfig(config);\n  }\n\n  /**\n   * Create a standardized LLM error\n   */\n  protected createError(\n    code: string,\n    message: string,\n    retryable: boolean = false,\n    statusCode?: number,\n    details?: unknown\n  ): LLMError {\n    const error: LLMError = {\n      code,\n      message,\n      retryable,\n      ...(statusCode !== undefined && { statusCode }),\n      ...(details !== undefined && { details }),\n    };\n    \n    return error;\n  }\n\n  /**\n   * Handle HTTP response errors\n   */\n  protected async handleHTTPError(response: Response): Promise<LLMError> {\n    let errorMessage = `HTTP ${response.status}: ${response.statusText}`;\n    let details: unknown;\n\n    try {\n      const errorBody = await response.text();\n      if (errorBody) {\n        try {\n          details = JSON.parse(errorBody);\n          // Try to extract a more specific error message\n          if (typeof details === 'object' && details !== null) {\n            const errorObj = details as Record<string, unknown>;\n            if (errorObj['error'] && typeof errorObj['error'] === 'object') {\n              const error = errorObj['error'] as Record<string, unknown>;\n              if (typeof error['message'] === 'string') {\n                errorMessage = error['message'];\n              }\n            } else if (typeof errorObj['message'] === 'string') {\n              errorMessage = errorObj['message'];\n            }\n          }\n        } catch {\n          // If JSON parsing fails, use the raw text as details\n          details = errorBody;\n        }\n      }\n    } catch {\n      // If reading response body fails, use default message\n    }\n\n    const retryable = response.status >= 500 || response.status === 429;\n    \n    return this.createError(\n      `HTTP_${response.status}`,\n      errorMessage,\n      retryable,\n      response.status,\n      details\n    );\n  }\n\n  /**\n   * Handle network errors\n   */\n  protected handleNetworkError(error: unknown): LLMError {\n    const message = error instanceof Error ? error.message : 'Unknown network error';\n    return this.createError('NETWORK_ERROR', message, true, undefined, error);\n  }\n\n  /**\n   * Handle parsing errors\n   */\n  protected handleParsingError(error: unknown, data?: unknown): LLMError {\n    const message = error instanceof Error ? error.message : 'Failed to parse response';\n    return this.createError('PARSING_ERROR', message, false, undefined, { error, data });\n  }\n\n  /**\n   * Create HTTP headers for API requests\n   */\n  protected createHeaders(apiKey: string, additionalHeaders: Record<string, string> = {}): Record<string, string> {\n    return {\n      'Content-Type': 'application/json',\n      'User-Agent': 'Juxtaprompt/1.0.0',\n      ...additionalHeaders,\n      ...this.getAuthHeaders(apiKey),\n    };\n  }\n\n  /**\n   * Get authentication headers (to be implemented by subclasses)\n   */\n  protected abstract getAuthHeaders(apiKey: string): Record<string, string>;\n\n  /**\n   * Create request body for the API call\n   */\n  protected abstract createRequestBody(request: LLMRequest): Record<string, unknown>;\n\n  /**\n   * Parse streaming response chunk\n   */\n  protected abstract parseStreamChunk(data: string, requestId: string): LLMStreamChunk | null;\n\n  /**\n   * Validate streaming response chunk\n   */\n  protected abstract validateStreamChunk(data: unknown): ValidationResult<unknown>;\n\n  /**\n   * Process Server-Sent Events stream\n   */\n  protected async* processSSEStream(\n    response: Response, \n    requestId: string\n  ): AsyncIterable<LLMStreamChunk> {\n    const reader = response.body?.getReader();\n    if (!reader) {\n      throw new LLMProviderError(this.createError(\n        'NO_RESPONSE_BODY',\n        'No response body received',\n        false\n      ));\n    }\n\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          if (line.startsWith('data: ')) {\n            const data = line.slice(6);\n            if (data === '[DONE]') {\n              yield {\n                requestId,\n                content: '',\n                isComplete: true,\n              };\n              return;\n            }\n\n            try {\n              const chunk = this.parseStreamChunk(data, requestId);\n              if (chunk) {\n                yield chunk;\n              }\n            } catch (parseError) {\n              console.warn(`Failed to parse SSE data: ${data}`, parseError);\n              continue;\n            }\n          }\n        }\n      }\n    } finally {\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Make HTTP request with error handling\n   */\n  protected async makeRequest(\n    url: string, \n    options: RequestInit, \n    timeout: number = this.defaultTimeout\n  ): Promise<Response> {\n    const controller = new AbortController();\n    const timeoutId = setTimeout(() => controller.abort(), timeout);\n\n    try {\n      const response = await fetch(url, {\n        ...options,\n        signal: controller.signal,\n      });\n\n      clearTimeout(timeoutId);\n\n      if (!response.ok) {\n        const error = await this.handleHTTPError(response);\n        throw new LLMProviderError(error);\n      }\n\n      return response;\n    } catch (error) {\n      clearTimeout(timeoutId);\n      \n      if (error instanceof LLMProviderError) {\n        throw error;\n      }\n      \n      if (error instanceof Error && error.name === 'AbortError') {\n        throw new LLMProviderError(this.createError(\n          'TIMEOUT_ERROR',\n          `Request timed out after ${timeout}ms`,\n          true\n        ));\n      }\n      \n      const networkError = this.handleNetworkError(error);\n      throw new LLMProviderError(networkError);\n    }\n  }\n\n  /**\n   * Generate unique request ID\n   */\n  protected generateRequestId(): string {\n    return `${this.name}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  /**\n   * Log request for debugging (can be overridden)\n   */\n  protected logRequest(request: LLMRequest): void {\n    // Only log in development mode (when not in production build)\n    if (typeof window !== 'undefined' && window.location.hostname === 'localhost') {\n      console.log(`[${this.name}] Sending request:`, {\n        id: request.id,\n        prompt: request.prompt.substring(0, 100) + '...',\n        model: request.config.model,\n        timestamp: request.timestamp,\n      });\n    }\n  }\n\n  /**\n   * Log response for debugging (can be overridden)\n   */\n  protected logResponse(chunk: LLMStreamChunk): void {\n    // Only log in development mode (when not in production build)\n    if (typeof window !== 'undefined' && window.location.hostname === 'localhost' && chunk.isComplete) {\n      console.log(`[${this.name}] Request completed:`, {\n        requestId: chunk.requestId,\n        tokenCount: chunk.tokenCount,\n      });\n    }\n  }\n}","// src/services/llm/llm-provider-manager.ts\nimport type {\n  ILLMProvider,\n  LLMProvider,\n  LLMRequest,\n  LLMStreamChunk,\n  LLMConfig,\n  ValidationResult,\n  AvailableModelsResult\n} from '@/types/llm';\nimport { LLMProviderError } from './base-llm-provider';\n\n/**\n * Manager class for handling multiple LLM providers\n */\nexport class LLMProviderManager {\n  private readonly providers = new Map<LLMProvider, ILLMProvider>();\n  private readonly activeRequests = new Map<string, AbortController>();\n\n  /**\n   * Register an LLM provider\n   */\n  public registerProvider(provider: ILLMProvider): void {\n    this.providers.set(provider.name, provider);\n  }\n\n  /**\n   * Get a registered provider\n   */\n  public getProvider(name: LLMProvider): ILLMProvider | null {\n    return this.providers.get(name) ?? null;\n  }\n\n  /**\n   * Get all registered provider names\n   */\n  public getRegisteredProviders(): LLMProvider[] {\n    return Array.from(this.providers.keys());\n  }\n\n  /**\n   * Check if a provider is registered\n   */\n  public hasProvider(name: LLMProvider): boolean {\n    return this.providers.has(name);\n  }\n\n  /**\n   * Send streaming request using the specified provider\n   */\n  public async* sendStreamingRequest(request: LLMRequest): AsyncIterable<LLMStreamChunk> {\n    const provider = this.getProvider(request.config.provider);\n    if (!provider) {\n      throw new LLMProviderError({\n        code: 'PROVIDER_NOT_FOUND',\n        message: `Provider '${request.config.provider}' is not registered`,\n        retryable: false,\n      });\n    }\n\n    // Validate configuration\n    const validation = provider.validateConfig(request.config);\n    if (!validation.success) {\n      throw new LLMProviderError({\n        code: 'INVALID_CONFIG',\n        message: `Invalid configuration: ${validation.error.message}`,\n        retryable: false,\n        details: validation.error.details,\n      });\n    }\n\n    // Create abort controller for this request\n    const controller = new AbortController();\n    this.activeRequests.set(request.id, controller);\n\n    try {\n      // Send the request\n      for await (const chunk of provider.sendStreamingRequest(request)) {\n        // Check if request was cancelled\n        if (controller.signal.aborted) {\n          yield {\n            requestId: request.id,\n            content: '',\n            isComplete: true,\n          };\n          return;\n        }\n\n        yield chunk;\n      }\n    } finally {\n      // Clean up\n      this.activeRequests.delete(request.id);\n    }\n  }\n\n  /**\n   * Cancel an active request\n   */\n  public cancelRequest(requestId: string): boolean {\n    const controller = this.activeRequests.get(requestId);\n    if (controller) {\n      controller.abort();\n      this.activeRequests.delete(requestId);\n      return true;\n    }\n    return false;\n  }\n\n  /**\n   * Cancel all active requests\n   */\n  public cancelAllRequests(): void {\n    for (const controller of this.activeRequests.values()) {\n      controller.abort();\n    }\n    this.activeRequests.clear();\n  }\n\n  /**\n   * Get the number of active requests\n   */\n  public getActiveRequestCount(): number {\n    return this.activeRequests.size;\n  }\n\n  /**\n   * Get active request IDs\n   */\n  public getActiveRequestIds(): string[] {\n    return Array.from(this.activeRequests.keys());\n  }\n\n  /**\n   * Get available models for a specific provider\n   */\n  public async getAvailableModels(\n    providerName: LLMProvider,\n    apiKey: string,\n    baseUrl?: string\n  ): Promise<AvailableModelsResult> {\n    const provider = this.getProvider(providerName);\n    if (!provider) {\n      throw new LLMProviderError({\n        code: 'PROVIDER_NOT_FOUND',\n        message: `Provider '${providerName}' is not registered`,\n        retryable: false,\n      });\n    }\n\n    return provider.getAvailableModels(apiKey, baseUrl);\n  }\n\n  /**\n   * Clear model cache for a specific provider\n   */\n  public clearModelCache(providerName: LLMProvider): void {\n    const provider = this.getProvider(providerName);\n    if (provider && 'clearModelCache' in provider) {\n      (provider as any).clearModelCache();\n    }\n  }\n\n  /**\n   * Clear model cache for all providers\n   */\n  public clearAllModelCaches(): void {\n    for (const provider of this.providers.values()) {\n      if ('clearModelCache' in provider) {\n        (provider as any).clearModelCache();\n      }\n    }\n  }\n\n  /**\n   * Validate configuration for a specific provider\n   */\n  public validateConfig(providerName: LLMProvider, config: Partial<LLMConfig>): ValidationResult<LLMConfig> {\n    const provider = this.getProvider(providerName);\n    if (!provider) {\n      return {\n        success: false,\n        error: {\n          code: 'PROVIDER_NOT_FOUND',\n          message: `Provider '${providerName}' is not registered`,\n        },\n      };\n    }\n\n    return provider.validateConfig(config);\n  }\n\n  /**\n   * Create a request object with validation\n   */\n  public createRequest(\n    prompt: string, \n    config: LLMConfig, \n    id?: string\n  ): ValidationResult<LLMRequest> {\n    // Validate the configuration\n    const validation = this.validateConfig(config.provider, config);\n    if (!validation.success) {\n      return validation as ValidationResult<LLMRequest>;\n    }\n\n    const request: LLMRequest = {\n      id: id ?? this.generateRequestId(config.provider),\n      prompt,\n      config: validation.data,\n      timestamp: Date.now(),\n    };\n\n    return {\n      success: true,\n      data: request,\n    };\n  }\n\n  /**\n   * Generate a unique request ID\n   */\n  private generateRequestId(provider: LLMProvider): string {\n    return `${provider}_${Date.now()}_${Math.random().toString(36).substr(2, 9)}`;\n  }\n\n  /**\n   * Get provider statistics\n   */\n  public getProviderStats(): Record<LLMProvider, {\n    registered: boolean;\n    activeRequests: number;\n  }> {\n    const stats: Record<string, { registered: boolean; activeRequests: number }> = {};\n    \n    // Initialize all known providers\n    const allProviders: LLMProvider[] = ['openai', 'anthropic', 'gemini'];\n    for (const provider of allProviders) {\n      stats[provider] = {\n        registered: this.hasProvider(provider),\n        activeRequests: 0,\n      };\n    }\n\n    // Count active requests per provider\n    for (const requestId of this.activeRequests.keys()) {\n      const provider = requestId.split('_')[0] as LLMProvider;\n      if (stats[provider]) {\n        stats[provider].activeRequests++;\n      }\n    }\n\n    return stats as Record<LLMProvider, { registered: boolean; activeRequests: number }>;\n  }\n\n  /**\n   * Clean up resources\n   */\n  public dispose(): void {\n    this.cancelAllRequests();\n    this.providers.clear();\n  }\n}\n\n// Export singleton instance\nexport const llmProviderManager = new LLMProviderManager();","// src/services/llm/providers/openai-provider.ts\nimport { BaseLLMProvider, LLMProviderError } from '../base-llm-provider';\nimport type { LLMRequest, LLMStreamChunk, ValidationResult, ModelInfo } from '@/types/llm';\nimport { validateOpenAIResponse, type OpenAIStreamChunk } from '@/schemas/llm-schemas';\n\n/**\n * OpenAI provider implementation with streaming support and validation\n */\nexport class OpenAIProvider extends BaseLLMProvider {\n  public readonly name = 'openai' as const;\n\n  /**\n   * Send streaming request to OpenAI API\n   */\n  public async* sendStreamingRequest(request: LLMRequest): AsyncIterable<LLMStreamChunk> {\n    this.logRequest(request);\n\n    const url = `${request.config.baseUrl}/chat/completions`;\n    const headers = this.createHeaders(request.config.apiKey);\n    const body = this.createRequestBody(request);\n\n    try {\n      const response = await this.makeRequest(url, {\n        method: 'POST',\n        headers,\n        body: JSON.stringify(body),\n      });\n\n      yield* this.processSSEStream(response, request.id);\n    } catch (error) {\n      if (error instanceof LLMProviderError) {\n        throw error;\n      }\n      \n      const networkError = this.handleNetworkError(error);\n      throw new LLMProviderError(networkError);\n    }\n  }\n\n  /**\n   * Get authentication headers for OpenAI\n   */\n  protected getAuthHeaders(apiKey: string): Record<string, string> {\n    return {\n      'Authorization': `Bearer ${apiKey}`,\n    };\n  }\n\n  /**\n   * Create request body for OpenAI API\n   */\n  protected createRequestBody(request: LLMRequest): Record<string, unknown> {\n    const messages: Array<{ role: string; content: string }> = [];\n    \n    // Add system message if provided\n    if (request.config.systemMessage.trim()) {\n      messages.push({\n        role: 'system',\n        content: request.config.systemMessage,\n      });\n    }\n    \n    // Add user message\n    messages.push({\n      role: 'user',\n      content: request.prompt,\n    });\n\n    return {\n      model: request.config.model,\n      messages,\n      temperature: request.config.temperature,\n      max_tokens: request.config.maxTokens,\n      top_p: request.config.topP,\n      frequency_penalty: request.config.frequencyPenalty,\n      presence_penalty: request.config.presencePenalty,\n      stream: true,\n    };\n  }\n\n  /**\n   * Parse streaming response chunk from OpenAI\n   */\n  protected parseStreamChunk(data: string, requestId: string): LLMStreamChunk | null {\n    try {\n      const parsed = JSON.parse(data);\n      const validation = this.validateStreamChunk(parsed);\n      \n      if (!validation.success) {\n        console.warn('Invalid OpenAI response format:', validation.error);\n        return null;\n      }\n\n      const chunk = validation.data as OpenAIStreamChunk;\n      const choice = chunk.choices[0];\n      \n      if (!choice) {\n        return null;\n      }\n\n      const content = choice.delta.content || '';\n      const isComplete = choice.finish_reason !== null;\n\n      const streamChunk: LLMStreamChunk = {\n        requestId,\n        content,\n        isComplete,\n        ...(isComplete && (parsed as any).usage?.total_tokens && {\n          tokenCount: (parsed as any).usage.total_tokens\n        }),\n      };\n\n      this.logResponse(streamChunk);\n      return streamChunk;\n    } catch (error) {\n      console.warn('Failed to parse OpenAI stream chunk:', error);\n      return null;\n    }\n  }\n\n  /**\n   * Validate streaming response chunk\n   */\n  protected validateStreamChunk(data: unknown): ValidationResult<OpenAIStreamChunk> {\n    return validateOpenAIResponse(data);\n  }\n\n  /**\n   * Process Server-Sent Events stream for OpenAI\n   */\n  protected override async* processSSEStream(\n    response: Response, \n    requestId: string\n  ): AsyncIterable<LLMStreamChunk> {\n    const reader = response.body?.getReader();\n    if (!reader) {\n      throw new LLMProviderError(this.createError(\n        'NO_RESPONSE_BODY',\n        'No response body received from OpenAI',\n        false\n      ));\n    }\n\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          const trimmedLine = line.trim();\n          \n          if (trimmedLine.startsWith('data: ')) {\n            const data = trimmedLine.slice(6);\n            \n            // Check for stream end\n            if (data === '[DONE]') {\n              yield {\n                requestId,\n                content: '',\n                isComplete: true,\n              };\n              return;\n            }\n\n            // Parse and yield chunk\n            const chunk = this.parseStreamChunk(data, requestId);\n            if (chunk) {\n              yield chunk;\n              \n              // If this chunk indicates completion, stop processing\n              if (chunk.isComplete) {\n                return;\n              }\n            }\n          }\n        }\n      }\n    } catch (error) {\n      const parseError = this.handleParsingError(error);\n      throw new LLMProviderError(parseError);\n    } finally {\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Handle OpenAI-specific errors\n   */\n  protected override async handleHTTPError(response: Response): Promise<import('@/types/llm').LLMError> {\n    let errorMessage = `OpenAI API error: ${response.status} ${response.statusText}`;\n    let details: unknown;\n\n    try {\n      const errorBody = await response.text();\n      if (errorBody) {\n        try {\n          const errorData = JSON.parse(errorBody);\n          details = errorData;\n          \n          // Extract OpenAI-specific error message\n          if (errorData.error?.message) {\n            errorMessage = errorData.error.message;\n          }\n        } catch {\n          details = errorBody;\n        }\n      }\n    } catch {\n      // Use default message if we can't read the response\n    }\n\n    // Determine if error is retryable\n    const retryable = response.status >= 500 || \n                     response.status === 429 || \n                     response.status === 408;\n\n    return this.createError(\n      `OPENAI_HTTP_${response.status}`,\n      errorMessage,\n      retryable,\n      response.status,\n      details\n    );\n  }\n\n  /**\n   * Fetch available models from OpenAI API\n   */\n  protected async fetchAvailableModels(apiKey: string, baseUrl?: string): Promise<ModelInfo[]> {\n    const url = `${baseUrl || 'https://api.openai.com/v1'}/models`;\n    const headers = this.createHeaders(apiKey);\n\n    try {\n      const response = await this.makeRequest(url, {\n        method: 'GET',\n        headers,\n      });\n\n      const data = await response.json();\n      \n      if (!data.data || !Array.isArray(data.data)) {\n        throw new Error('Invalid response format from OpenAI models API');\n      }\n\n      // Filter and map OpenAI models to our ModelInfo format\n      const models: ModelInfo[] = data.data\n        .filter((model: any) => {\n          // Only include chat completion models\n          return model.id && (\n            model.id.startsWith('gpt-') ||\n            model.id.includes('chat') ||\n            model.id.includes('turbo')\n          );\n        })\n        .map((model: any) => ({\n          id: model.id,\n          name: model.id,\n          description: this.getModelDescription(model.id),\n          contextLength: this.getModelContextLength(model.id),\n          maxOutputTokens: this.getModelMaxOutputTokens(model.id),\n          pricing: this.getModelPricing(model.id),\n        }))\n        .sort((a: ModelInfo, b: ModelInfo) => {\n          // Sort by preference: gpt-4o models first, then gpt-4, then gpt-3.5\n          const getModelPriority = (id: string) => {\n            if (id.includes('gpt-4o')) return 1;\n            if (id.includes('gpt-4')) return 2;\n            if (id.includes('gpt-3.5')) return 3;\n            return 4;\n          };\n          return getModelPriority(a.id) - getModelPriority(b.id);\n        });\n\n      return models;\n    } catch (error) {\n      console.error('Failed to fetch OpenAI models:', error);\n      throw error;\n    }\n  }\n\n  /**\n   * Get fallback models when API call fails\n   */\n  protected getFallbackModels(): ModelInfo[] {\n    return [\n      {\n        id: 'gpt-4o',\n        name: 'GPT-4o',\n        description: 'Most advanced multimodal model',\n        contextLength: 128000,\n        maxOutputTokens: 4096,\n        pricing: { input: 5.0, output: 15.0 },\n      },\n      {\n        id: 'gpt-4o-mini',\n        name: 'GPT-4o Mini',\n        description: 'Affordable and intelligent small model',\n        contextLength: 128000,\n        maxOutputTokens: 16384,\n        pricing: { input: 0.15, output: 0.6 },\n      },\n      {\n        id: 'gpt-4-turbo',\n        name: 'GPT-4 Turbo',\n        description: 'Previous generation flagship model',\n        contextLength: 128000,\n        maxOutputTokens: 4096,\n        pricing: { input: 10.0, output: 30.0 },\n      },\n      {\n        id: 'gpt-4',\n        name: 'GPT-4',\n        description: 'Original GPT-4 model',\n        contextLength: 8192,\n        maxOutputTokens: 4096,\n        pricing: { input: 30.0, output: 60.0 },\n      },\n      {\n        id: 'gpt-3.5-turbo',\n        name: 'GPT-3.5 Turbo',\n        description: 'Fast and affordable model',\n        contextLength: 16385,\n        maxOutputTokens: 4096,\n        pricing: { input: 0.5, output: 1.5 },\n      },\n    ];\n  }\n\n  /**\n   * Get model description\n   */\n  private getModelDescription(modelId: string): string {\n    const descriptions: Record<string, string> = {\n      'gpt-4o': 'Most advanced multimodal model',\n      'gpt-4o-mini': 'Affordable and intelligent small model',\n      'gpt-4-turbo': 'Previous generation flagship model',\n      'gpt-4': 'Original GPT-4 model',\n      'gpt-3.5-turbo': 'Fast and affordable model',\n    };\n    return descriptions[modelId] || 'OpenAI language model';\n  }\n\n  /**\n   * Get model context length\n   */\n  private getModelContextLength(modelId: string): number {\n    const contextLengths: Record<string, number> = {\n      'gpt-4o': 128000,\n      'gpt-4o-mini': 128000,\n      'gpt-4-turbo': 128000,\n      'gpt-4': 8192,\n      'gpt-3.5-turbo': 16385,\n    };\n    return contextLengths[modelId] || 4096;\n  }\n\n  /**\n   * Get model max output tokens\n   */\n  private getModelMaxOutputTokens(modelId: string): number {\n    const maxOutputTokens: Record<string, number> = {\n      'gpt-4o': 4096,\n      'gpt-4o-mini': 16384,\n      'gpt-4-turbo': 4096,\n      'gpt-4': 4096,\n      'gpt-3.5-turbo': 4096,\n    };\n    return maxOutputTokens[modelId] || 4096;\n  }\n\n  /**\n   * Get model pricing (per 1M tokens)\n   */\n  private getModelPricing(modelId: string): { input: number; output: number } {\n    const pricing: Record<string, { input: number; output: number }> = {\n      'gpt-4o': { input: 5.0, output: 15.0 },\n      'gpt-4o-mini': { input: 0.15, output: 0.6 },\n      'gpt-4-turbo': { input: 10.0, output: 30.0 },\n      'gpt-4': { input: 30.0, output: 60.0 },\n      'gpt-3.5-turbo': { input: 0.5, output: 1.5 },\n    };\n    return pricing[modelId] || { input: 1.0, output: 2.0 };\n  }\n\n  /**\n   * Validate OpenAI-specific configuration\n   */\n  public override validateConfig(config: Partial<import('@/types/llm').LLMConfig>): ValidationResult<import('@/types/llm').LLMConfig> {\n    // First run base validation\n    const baseValidation = super.validateConfig(config);\n    if (!baseValidation.success) {\n      return baseValidation;\n    }\n\n    const validatedConfig = baseValidation.data;\n\n    // OpenAI-specific validations\n    if (validatedConfig.provider !== 'openai') {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_PROVIDER',\n          message: 'Provider must be \"openai\" for OpenAI provider',\n        },\n      };\n    }\n\n    // Validate API key format (OpenAI keys start with 'sk-')\n    if (!validatedConfig.apiKey.startsWith('sk-')) {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_API_KEY',\n          message: 'OpenAI API key must start with \"sk-\"',\n        },\n      };\n    }\n\n    // Validate model is supported\n    const supportedModels = [\n      'gpt-4o',\n      'gpt-4o-mini',\n      'gpt-4-turbo',\n      'gpt-4',\n      'gpt-3.5-turbo',\n    ];\n    \n    if (!supportedModels.includes(validatedConfig.model)) {\n      return {\n        success: false,\n        error: {\n          code: 'UNSUPPORTED_MODEL',\n          message: `Model \"${validatedConfig.model}\" is not supported. Supported models: ${supportedModels.join(', ')}`,\n        },\n      };\n    }\n\n    // Validate base URL\n    const validBaseUrls = [\n      'https://api.openai.com/v1',\n      'https://api.openai.com/v1/',\n    ];\n    \n    const normalizedBaseUrl = validatedConfig.baseUrl.replace(/\\/$/, '');\n    if (!validBaseUrls.some(url => url.replace(/\\/$/, '') === normalizedBaseUrl)) {\n      console.warn(`Non-standard OpenAI base URL: ${validatedConfig.baseUrl}`);\n    }\n\n    return {\n      success: true,\n      data: validatedConfig,\n    };\n  }\n}\n\n// Export singleton instance\nexport const openaiProvider = new OpenAIProvider();","// src/services/llm/providers/anthropic-provider.ts\nimport { BaseLLMProvider, LLMProviderError } from '../base-llm-provider';\nimport type { LLMRequest, LLMStreamChunk, ValidationResult, ModelInfo } from '@/types/llm';\nimport { validateAnthropicResponse, type AnthropicStreamChunk } from '@/schemas/llm-schemas';\n\n/**\n * Anthropic provider implementation with streaming support and validation\n */\nexport class AnthropicProvider extends BaseLLMProvider {\n  public readonly name = 'anthropic' as const;\n\n  /**\n   * Send streaming request to Anthropic API\n   */\n  public async* sendStreamingRequest(request: LLMRequest): AsyncIterable<LLMStreamChunk> {\n    this.logRequest(request);\n\n    const url = `${request.config.baseUrl}/v1/messages`;\n    const headers = this.createHeaders(request.config.apiKey, {\n      'anthropic-version': '2023-06-01',\n    });\n    const body = this.createRequestBody(request);\n\n    try {\n      const response = await this.makeRequest(url, {\n        method: 'POST',\n        headers,\n        body: JSON.stringify(body),\n      });\n\n      yield* this.processSSEStream(response, request.id);\n    } catch (error) {\n      if (error instanceof LLMProviderError) {\n        throw error;\n      }\n      \n      const networkError = this.handleNetworkError(error);\n      throw new LLMProviderError(networkError);\n    }\n  }\n\n  /**\n   * Get authentication headers for Anthropic\n   */\n  protected getAuthHeaders(apiKey: string): Record<string, string> {\n    return {\n      'x-api-key': apiKey,\n    };\n  }\n\n  /**\n   * Create request body for Anthropic API\n   */\n  protected createRequestBody(request: LLMRequest): Record<string, unknown> {\n    const messages: Array<{ role: string; content: string }> = [];\n    \n    // Add user message\n    messages.push({\n      role: 'user',\n      content: request.prompt,\n    });\n\n    const body: Record<string, unknown> = {\n      model: request.config.model,\n      messages,\n      max_tokens: request.config.maxTokens,\n      temperature: request.config.temperature,\n      top_p: request.config.topP,\n      stream: true,\n    };\n\n    // Add system message if provided\n    if (request.config.systemMessage.trim()) {\n      body['system'] = request.config.systemMessage;\n    }\n\n    return body;\n  }\n\n  /**\n   * Parse streaming response chunk from Anthropic\n   */\n  protected parseStreamChunk(data: string, requestId: string): LLMStreamChunk | null {\n    try {\n      const parsed = JSON.parse(data);\n      const validation = this.validateStreamChunk(parsed);\n      \n      if (!validation.success) {\n        console.warn('Invalid Anthropic response format:', validation.error);\n        return null;\n      }\n\n      const chunk = validation.data as AnthropicStreamChunk;\n      \n      // Handle different event types\n      switch (chunk.type) {\n        case 'message_start':\n          return {\n            requestId,\n            content: '',\n            isComplete: false,\n          };\n\n        case 'content_block_delta':\n          if (chunk.delta?.text) {\n            return {\n              requestId,\n              content: chunk.delta.text,\n              isComplete: false,\n            };\n          }\n          return null;\n\n        case 'message_delta':\n          if (chunk.delta?.stop_reason) {\n            const streamChunk: LLMStreamChunk = {\n              requestId,\n              content: '',\n              isComplete: true,\n            };\n\n            // Add token count if available\n            if (chunk.message?.usage?.output_tokens) {\n              return {\n                ...streamChunk,\n                tokenCount: chunk.message.usage.input_tokens + chunk.message.usage.output_tokens,\n              };\n            }\n\n            this.logResponse(streamChunk);\n            return streamChunk;\n          }\n          return null;\n\n        case 'message_stop': {\n          const completionChunk: LLMStreamChunk = {\n            requestId,\n            content: '',\n            isComplete: true,\n          };\n          \n          this.logResponse(completionChunk);\n          return completionChunk;\n        }\n\n        default:\n          return null;\n      }\n    } catch (error) {\n      console.warn('Failed to parse Anthropic stream chunk:', error);\n      return null;\n    }\n  }\n\n  /**\n   * Validate streaming response chunk\n   */\n  protected validateStreamChunk(data: unknown): ValidationResult<AnthropicStreamChunk> {\n    return validateAnthropicResponse(data);\n  }\n\n  /**\n   * Process Server-Sent Events stream for Anthropic\n   */\n  protected override async* processSSEStream(\n    response: Response, \n    requestId: string\n  ): AsyncIterable<LLMStreamChunk> {\n    const reader = response.body?.getReader();\n    if (!reader) {\n      throw new LLMProviderError(this.createError(\n        'NO_RESPONSE_BODY',\n        'No response body received from Anthropic',\n        false\n      ));\n    }\n\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          const trimmedLine = line.trim();\n          \n          if (trimmedLine.startsWith('data: ')) {\n            const data = trimmedLine.slice(6);\n            \n            // Skip empty data or ping events\n            if (!data || data === '{}') {\n              continue;\n            }\n\n            // Parse and yield chunk\n            const chunk = this.parseStreamChunk(data, requestId);\n            if (chunk) {\n              yield chunk;\n              \n              // If this chunk indicates completion, stop processing\n              if (chunk.isComplete) {\n                return;\n              }\n            }\n          } else if (trimmedLine.startsWith('event: ')) {\n            // Handle event types if needed\n            const eventType = trimmedLine.slice(7);\n            if (eventType === 'message_stop') {\n              yield {\n                requestId,\n                content: '',\n                isComplete: true,\n              };\n              return;\n            }\n          }\n        }\n      }\n    } catch (error) {\n      const parseError = this.handleParsingError(error);\n      throw new LLMProviderError(parseError);\n    } finally {\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Handle Anthropic-specific errors\n   */\n  protected override async handleHTTPError(response: Response): Promise<import('@/types/llm').LLMError> {\n    let errorMessage = `Anthropic API error: ${response.status} ${response.statusText}`;\n    let details: unknown;\n\n    try {\n      const errorBody = await response.text();\n      if (errorBody) {\n        try {\n          const errorData = JSON.parse(errorBody);\n          details = errorData;\n          \n          // Extract Anthropic-specific error message\n          if (errorData.error?.message) {\n            errorMessage = errorData.error.message;\n          } else if (errorData.message) {\n            errorMessage = errorData.message;\n          }\n        } catch {\n          details = errorBody;\n        }\n      }\n    } catch {\n      // Use default message if we can't read the response\n    }\n\n    // Determine if error is retryable\n    const retryable = response.status >= 500 || \n                     response.status === 429 || \n                     response.status === 408;\n\n    return this.createError(\n      `ANTHROPIC_HTTP_${response.status}`,\n      errorMessage,\n      retryable,\n      response.status,\n      details\n    );\n  }\n\n  /**\n   * Fetch available models from Anthropic API\n   * Note: Anthropic doesn't provide a public models endpoint, so we return static models\n   */\n  protected async fetchAvailableModels(): Promise<ModelInfo[]> {\n    // Anthropic doesn't have a public models API endpoint\n    // We could potentially make a test request to validate the API key\n    // but for now, we'll return the static models\n    return this.getFallbackModels();\n  }\n\n  /**\n   * Get fallback models when API call fails\n   */\n  protected getFallbackModels(): ModelInfo[] {\n    return [\n      {\n        id: 'claude-3-5-sonnet-20241022',\n        name: 'Claude 3.5 Sonnet',\n        description: 'Most intelligent model, ideal for complex tasks',\n        contextLength: 200000,\n        maxOutputTokens: 8192,\n        pricing: { input: 3.0, output: 15.0 },\n      },\n      {\n        id: 'claude-3-5-haiku-20241022',\n        name: 'Claude 3.5 Haiku',\n        description: 'Fastest model for everyday tasks',\n        contextLength: 200000,\n        maxOutputTokens: 8192,\n        pricing: { input: 0.8, output: 4.0 },\n      },\n      {\n        id: 'claude-3-opus-20240229',\n        name: 'Claude 3 Opus',\n        description: 'Powerful model for highly complex tasks',\n        contextLength: 200000,\n        maxOutputTokens: 4096,\n        pricing: { input: 15.0, output: 75.0 },\n      },\n      {\n        id: 'claude-3-sonnet-20240229',\n        name: 'Claude 3 Sonnet',\n        description: 'Balance of intelligence and speed',\n        contextLength: 200000,\n        maxOutputTokens: 4096,\n        pricing: { input: 3.0, output: 15.0 },\n      },\n      {\n        id: 'claude-3-haiku-20240307',\n        name: 'Claude 3 Haiku',\n        description: 'Fast and cost-effective model',\n        contextLength: 200000,\n        maxOutputTokens: 4096,\n        pricing: { input: 0.25, output: 1.25 },\n      },\n    ];\n  }\n\n  /**\n   * Validate Anthropic-specific configuration\n   */\n  public override validateConfig(config: Partial<import('@/types/llm').LLMConfig>): ValidationResult<import('@/types/llm').LLMConfig> {\n    // First run base validation\n    const baseValidation = super.validateConfig(config);\n    if (!baseValidation.success) {\n      return baseValidation;\n    }\n\n    const validatedConfig = baseValidation.data;\n\n    // Anthropic-specific validations\n    if (validatedConfig.provider !== 'anthropic') {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_PROVIDER',\n          message: 'Provider must be \"anthropic\" for Anthropic provider',\n        },\n      };\n    }\n\n    // Validate API key format (Anthropic keys start with 'sk-ant-')\n    if (!validatedConfig.apiKey.startsWith('sk-ant-')) {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_API_KEY',\n          message: 'Anthropic API key must start with \"sk-ant-\"',\n        },\n      };\n    }\n\n    // Validate model is supported\n    const supportedModels = [\n      'claude-3-5-sonnet-20241022',\n      'claude-3-5-haiku-20241022',\n      'claude-3-opus-20240229',\n      'claude-3-sonnet-20240229',\n      'claude-3-haiku-20240307',\n    ];\n    \n    if (!supportedModels.includes(validatedConfig.model)) {\n      return {\n        success: false,\n        error: {\n          code: 'UNSUPPORTED_MODEL',\n          message: `Model \"${validatedConfig.model}\" is not supported. Supported models: ${supportedModels.join(', ')}`,\n        },\n      };\n    }\n\n    // Validate base URL\n    const validBaseUrls = [\n      'https://api.anthropic.com',\n      'https://api.anthropic.com/',\n    ];\n    \n    const normalizedBaseUrl = validatedConfig.baseUrl.replace(/\\/$/, '');\n    if (!validBaseUrls.some(url => url.replace(/\\/$/, '') === normalizedBaseUrl)) {\n      console.warn(`Non-standard Anthropic base URL: ${validatedConfig.baseUrl}`);\n    }\n\n    // Validate max tokens (Anthropic has different limits)\n    if (validatedConfig.maxTokens > 8192) {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_MAX_TOKENS',\n          message: 'Anthropic models support a maximum of 8192 tokens',\n        },\n      };\n    }\n\n    return {\n      success: true,\n      data: validatedConfig,\n    };\n  }\n}\n\n// Export singleton instance\nexport const anthropicProvider = new AnthropicProvider();","// src/services/llm/providers/gemini-provider.ts\nimport { BaseLLMProvider, LLMProviderError } from '../base-llm-provider';\nimport type { LLMRequest, LLMStreamChunk, ValidationResult, ModelInfo } from '@/types/llm';\nimport { validateGeminiResponse, type GeminiStreamChunk } from '@/schemas/llm-schemas';\n\n/**\n * Google Gemini provider implementation with streaming support and validation\n */\nexport class GeminiProvider extends BaseLLMProvider {\n  public readonly name = 'gemini' as const;\n\n  /**\n   * Send streaming request to Gemini API\n   */\n  public async* sendStreamingRequest(request: LLMRequest): AsyncIterable<LLMStreamChunk> {\n    this.logRequest(request);\n\n    const url = `${request.config.baseUrl}/models/${request.config.model}:streamGenerateContent`;\n    const headers = this.createHeaders(request.config.apiKey);\n    const body = this.createRequestBody(request);\n\n    try {\n      const response = await this.makeRequest(url, {\n        method: 'POST',\n        headers,\n        body: JSON.stringify(body),\n      });\n\n      yield* this.processSSEStream(response, request.id);\n    } catch (error) {\n      if (error instanceof LLMProviderError) {\n        throw error;\n      }\n      \n      const networkError = this.handleNetworkError(error);\n      throw new LLMProviderError(networkError);\n    }\n  }\n\n  /**\n   * Get authentication headers for Gemini\n   */\n  protected getAuthHeaders(): Record<string, string> {\n    // Gemini uses API key as query parameter, not in headers\n    return {};\n  }\n\n  /**\n   * Create request body for Gemini API\n   */\n  protected createRequestBody(request: LLMRequest): Record<string, unknown> {\n    const contents: Array<{ role: string; parts: Array<{ text: string }> }> = [];\n    \n    // Add system instruction if provided\n    const systemInstruction = request.config.systemMessage.trim() \n      ? { parts: [{ text: request.config.systemMessage }] }\n      : undefined;\n    \n    // Add user message\n    contents.push({\n      role: 'user',\n      parts: [{ text: request.prompt }],\n    });\n\n    const body: Record<string, unknown> = {\n      contents,\n      generationConfig: {\n        temperature: request.config.temperature,\n        maxOutputTokens: request.config.maxTokens,\n        topP: request.config.topP,\n      },\n    };\n\n    if (systemInstruction) {\n      body['systemInstruction'] = systemInstruction;\n    }\n\n    return body;\n  }\n\n  /**\n   * Parse streaming response chunk from Gemini\n   */\n  protected parseStreamChunk(data: string, requestId: string): LLMStreamChunk | null {\n    try {\n      const parsed = JSON.parse(data);\n      const validation = this.validateStreamChunk(parsed);\n      \n      if (!validation.success) {\n        console.warn('Invalid Gemini response format:', validation.error);\n        return null;\n      }\n\n      const chunk = validation.data as GeminiStreamChunk;\n      const candidate = chunk.candidates?.[0];\n      \n      if (!candidate) {\n        return null;\n      }\n\n      const content = candidate.content?.parts?.[0]?.text || '';\n      const isComplete = candidate.finishReason !== undefined;\n\n      const streamChunk: LLMStreamChunk = {\n        requestId,\n        content,\n        isComplete,\n      };\n\n      if (isComplete) {\n        this.logResponse(streamChunk);\n      }\n\n      return streamChunk;\n    } catch (error) {\n      console.warn('Failed to parse Gemini stream chunk:', error);\n      return null;\n    }\n  }\n\n  /**\n   * Validate streaming response chunk\n   */\n  protected validateStreamChunk(data: unknown): ValidationResult<GeminiStreamChunk> {\n    return validateGeminiResponse(data);\n  }\n\n  /**\n   * Process Server-Sent Events stream for Gemini\n   */\n  protected override async* processSSEStream(\n    response: Response, \n    requestId: string\n  ): AsyncIterable<LLMStreamChunk> {\n    const reader = response.body?.getReader();\n    if (!reader) {\n      throw new LLMProviderError(this.createError(\n        'NO_RESPONSE_BODY',\n        'No response body received from Gemini',\n        false\n      ));\n    }\n\n    const decoder = new TextDecoder();\n    let buffer = '';\n\n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n\n        buffer += decoder.decode(value, { stream: true });\n        const lines = buffer.split('\\n');\n        buffer = lines.pop() || '';\n\n        for (const line of lines) {\n          const trimmedLine = line.trim();\n          \n          if (trimmedLine.startsWith('data: ')) {\n            const data = trimmedLine.slice(6);\n            \n            // Skip empty data\n            if (!data || data === '{}') {\n              continue;\n            }\n\n            // Parse and yield chunk\n            const chunk = this.parseStreamChunk(data, requestId);\n            if (chunk) {\n              yield chunk;\n              \n              // If this chunk indicates completion, stop processing\n              if (chunk.isComplete) {\n                return;\n              }\n            }\n          }\n        }\n      }\n    } catch (error) {\n      const parseError = this.handleParsingError(error);\n      throw new LLMProviderError(parseError);\n    } finally {\n      reader.releaseLock();\n    }\n  }\n\n  /**\n   * Make HTTP request with Gemini-specific URL parameters\n   */\n  protected override async makeRequest(\n    url: string, \n    options: RequestInit, \n    timeout: number = this.defaultTimeout\n  ): Promise<Response> {\n    // Add API key as query parameter for Gemini\n    const urlWithKey = new URL(url);\n    \n    // Extract API key from Authorization header if present\n    const headers = options.headers as Record<string, string> || {};\n    let apiKey = '';\n    \n    // Find API key in the request config (we need to extract it from the body)\n    if (options.body && typeof options.body === 'string') {\n      try {\n        // We'll get the API key from the headers we set earlier\n        // For Gemini, we need to add it as a query parameter\n        const authHeader = headers['Authorization'];\n        if (authHeader?.startsWith('Bearer ')) {\n          apiKey = authHeader.slice(7);\n        }\n      } catch {\n        // If we can't extract the API key, the request will fail with auth error\n      }\n    }\n    \n    if (apiKey) {\n      urlWithKey.searchParams.set('key', apiKey);\n      // Remove the Authorization header since Gemini uses query param\n      delete headers['Authorization'];\n    }\n\n    return super.makeRequest(urlWithKey.toString(), {\n      ...options,\n      headers,\n    }, timeout);\n  }\n\n  /**\n   * Get authentication headers for Gemini (override to include Bearer for extraction)\n   */\n  protected override createHeaders(apiKey: string, additionalHeaders: Record<string, string> = {}): Record<string, string> {\n    return {\n      'Content-Type': 'application/json',\n      'User-Agent': 'Juxtaprompt/1.0.0',\n      'Authorization': `Bearer ${apiKey}`, // Temporary for extraction\n      ...additionalHeaders,\n    };\n  }\n\n  /**\n   * Handle Gemini-specific errors\n   */\n  protected override async handleHTTPError(response: Response): Promise<import('@/types/llm').LLMError> {\n    let errorMessage = `Gemini API error: ${response.status} ${response.statusText}`;\n    let details: unknown;\n\n    try {\n      const errorBody = await response.text();\n      if (errorBody) {\n        try {\n          const errorData = JSON.parse(errorBody);\n          details = errorData;\n          \n          // Extract Gemini-specific error message\n          if (errorData.error?.message) {\n            errorMessage = errorData.error.message;\n          } else if (errorData.message) {\n            errorMessage = errorData.message;\n          }\n        } catch {\n          details = errorBody;\n        }\n      }\n    } catch {\n      // Use default message if we can't read the response\n    }\n\n    // Determine if error is retryable\n    const retryable = response.status >= 500 || \n                     response.status === 429 || \n                     response.status === 408;\n\n    return this.createError(\n      `GEMINI_HTTP_${response.status}`,\n      errorMessage,\n      retryable,\n      response.status,\n      details\n    );\n  }\n\n  /**\n   * Fetch available models from Gemini API\n   */\n  protected async fetchAvailableModels(apiKey: string, baseUrl?: string): Promise<ModelInfo[]> {\n    const url = `${baseUrl || 'https://generativelanguage.googleapis.com/v1beta'}/models`;\n    \n    try {\n      const response = await fetch(`${url}?key=${apiKey}`, {\n        method: 'GET',\n        headers: {\n          'Content-Type': 'application/json',\n          'User-Agent': 'Juxtaprompt/1.0.0',\n        },\n      });\n\n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n\n      const data = await response.json();\n      \n      if (!data.models || !Array.isArray(data.models)) {\n        throw new Error('Invalid response format from Gemini models API');\n      }\n\n      // Filter and map Gemini models to our ModelInfo format\n      const models: ModelInfo[] = data.models\n        .filter((model: any) => {\n          // Only include generative models that support generateContent\n          return model.name &&\n                 model.supportedGenerationMethods?.includes('generateContent') &&\n                 !model.name.includes('embedding');\n        })\n        .map((model: any) => {\n          const modelId = model.name.replace('models/', '');\n          return {\n            id: modelId,\n            name: this.getModelDisplayName(modelId),\n            description: model.description || this.getModelDescription(modelId),\n            contextLength: this.getModelContextLength(modelId),\n            maxOutputTokens: this.getModelMaxOutputTokens(modelId),\n            pricing: this.getModelPricing(modelId),\n          };\n        })\n        .sort((a: ModelInfo, b: ModelInfo) => {\n          // Sort by preference: gemini-1.5-pro first, then flash, then others\n          const getModelPriority = (id: string) => {\n            if (id.includes('gemini-1.5-pro')) return 1;\n            if (id.includes('gemini-1.5-flash')) return 2;\n            if (id.includes('gemini-1.0-pro')) return 3;\n            return 4;\n          };\n          return getModelPriority(a.id) - getModelPriority(b.id);\n        });\n\n      return models;\n    } catch (error) {\n      console.error('Failed to fetch Gemini models:', error);\n      throw error;\n    }\n  }\n\n  /**\n   * Get fallback models when API call fails\n   */\n  protected getFallbackModels(): ModelInfo[] {\n    return [\n      {\n        id: 'gemini-1.5-pro',\n        name: 'Gemini 1.5 Pro',\n        description: 'Most capable model for complex reasoning tasks',\n        contextLength: 2000000,\n        maxOutputTokens: 8192,\n        pricing: { input: 1.25, output: 5.0 },\n      },\n      {\n        id: 'gemini-1.5-flash',\n        name: 'Gemini 1.5 Flash',\n        description: 'Fast and efficient model for everyday tasks',\n        contextLength: 1000000,\n        maxOutputTokens: 8192,\n        pricing: { input: 0.075, output: 0.3 },\n      },\n      {\n        id: 'gemini-1.0-pro',\n        name: 'Gemini 1.0 Pro',\n        description: 'Previous generation model',\n        contextLength: 32768,\n        maxOutputTokens: 2048,\n        pricing: { input: 0.5, output: 1.5 },\n      },\n    ];\n  }\n\n  /**\n   * Get model display name\n   */\n  private getModelDisplayName(modelId: string): string {\n    const displayNames: Record<string, string> = {\n      'gemini-1.5-pro': 'Gemini 1.5 Pro',\n      'gemini-1.5-flash': 'Gemini 1.5 Flash',\n      'gemini-1.0-pro': 'Gemini 1.0 Pro',\n    };\n    return displayNames[modelId] || modelId;\n  }\n\n  /**\n   * Get model description\n   */\n  private getModelDescription(modelId: string): string {\n    const descriptions: Record<string, string> = {\n      'gemini-1.5-pro': 'Most capable model for complex reasoning tasks',\n      'gemini-1.5-flash': 'Fast and efficient model for everyday tasks',\n      'gemini-1.0-pro': 'Previous generation model',\n    };\n    return descriptions[modelId] || 'Google Gemini language model';\n  }\n\n  /**\n   * Get model context length\n   */\n  private getModelContextLength(modelId: string): number {\n    const contextLengths: Record<string, number> = {\n      'gemini-1.5-pro': 2000000,\n      'gemini-1.5-flash': 1000000,\n      'gemini-1.0-pro': 32768,\n    };\n    return contextLengths[modelId] || 32768;\n  }\n\n  /**\n   * Get model max output tokens\n   */\n  private getModelMaxOutputTokens(modelId: string): number {\n    const maxOutputTokens: Record<string, number> = {\n      'gemini-1.5-pro': 8192,\n      'gemini-1.5-flash': 8192,\n      'gemini-1.0-pro': 2048,\n    };\n    return maxOutputTokens[modelId] || 2048;\n  }\n\n  /**\n   * Get model pricing (per 1M tokens)\n   */\n  private getModelPricing(modelId: string): { input: number; output: number } {\n    const pricing: Record<string, { input: number; output: number }> = {\n      'gemini-1.5-pro': { input: 1.25, output: 5.0 },\n      'gemini-1.5-flash': { input: 0.075, output: 0.3 },\n      'gemini-1.0-pro': { input: 0.5, output: 1.5 },\n    };\n    return pricing[modelId] || { input: 0.5, output: 1.5 };\n  }\n\n  /**\n   * Validate Gemini-specific configuration\n   */\n  public override validateConfig(config: Partial<import('@/types/llm').LLMConfig>): ValidationResult<import('@/types/llm').LLMConfig> {\n    // First run base validation\n    const baseValidation = super.validateConfig(config);\n    if (!baseValidation.success) {\n      return baseValidation;\n    }\n\n    const validatedConfig = baseValidation.data;\n\n    // Gemini-specific validations\n    if (validatedConfig.provider !== 'gemini') {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_PROVIDER',\n          message: 'Provider must be \"gemini\" for Gemini provider',\n        },\n      };\n    }\n\n    // Validate API key format (Gemini keys are typically longer and alphanumeric)\n    if (validatedConfig.apiKey.length < 20) {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_API_KEY',\n          message: 'Gemini API key appears to be too short',\n        },\n      };\n    }\n\n    // Validate model is supported\n    const supportedModels = [\n      'gemini-1.5-pro',\n      'gemini-1.5-flash',\n      'gemini-1.0-pro',\n    ];\n    \n    if (!supportedModels.includes(validatedConfig.model)) {\n      return {\n        success: false,\n        error: {\n          code: 'UNSUPPORTED_MODEL',\n          message: `Model \"${validatedConfig.model}\" is not supported. Supported models: ${supportedModels.join(', ')}`,\n        },\n      };\n    }\n\n    // Validate base URL\n    const validBaseUrls = [\n      'https://generativelanguage.googleapis.com/v1beta',\n      'https://generativelanguage.googleapis.com/v1beta/',\n    ];\n    \n    const normalizedBaseUrl = validatedConfig.baseUrl.replace(/\\/$/, '');\n    if (!validBaseUrls.some(url => url.replace(/\\/$/, '') === normalizedBaseUrl)) {\n      console.warn(`Non-standard Gemini base URL: ${validatedConfig.baseUrl}`);\n    }\n\n    // Validate max tokens (Gemini has different limits)\n    if (validatedConfig.maxTokens > 8192) {\n      return {\n        success: false,\n        error: {\n          code: 'INVALID_MAX_TOKENS',\n          message: 'Gemini models support a maximum of 8192 output tokens',\n        },\n      };\n    }\n\n    return {\n      success: true,\n      data: validatedConfig,\n    };\n  }\n}\n\n// Export singleton instance\nexport const geminiProvider = new GeminiProvider();","// src/services/llm/index.ts\nexport * from './base-llm-provider';\nexport * from './llm-provider-manager';\nexport * from './providers';\n\n// Register all providers\nimport { llmProviderManager } from './llm-provider-manager';\nimport { openaiProvider } from './providers/openai-provider';\nimport { anthropicProvider } from './providers/anthropic-provider';\nimport { geminiProvider } from './providers/gemini-provider';\n\n// Register providers with the manager\nllmProviderManager.registerProvider(openaiProvider);\nllmProviderManager.registerProvider(anthropicProvider);\nllmProviderManager.registerProvider(geminiProvider);"],"names":["OpenAIStreamChunkSchema","z.object","id","z.string","object","z.literal","created","z.number","model","choices","z.array","index","delta","content","optional","role","finish_reason","nullable","error","message","type","param","code","AnthropicStreamChunkSchema","z.enum","text","stop_reason","stop_sequence","usage","input_tokens","output_tokens","content_block","GeminiStreamChunkSchema","candidates","parts","finishReason","safetyRatings","category","probability","promptFeedback","status","LLMConfigSchema","provider","apiKey","min","baseUrl","url","temperature","max","maxTokens","topP","frequencyPenalty","presencePenalty","systemMessage","URLStateSchema","prompts","config","ui","gridColumns","autoSend","z.boolean","debounceMs","showAdvancedSettings","theme","StoragePreferenceSchema","acknowledgedRisks","validateURLState","data","result","safeParse","success","details","issues","validateStoragePreference","LLMProviderError","Error","constructor","super","__publicField","this","name","retryable","statusCode","_BaseLLMProvider","getAvailableModels","cacheKey","slice","cached","modelCache","get","Date","now","timestamp","cacheTimeout","models","fetchAvailableModels","set","getFallbackModels","clearModelCache","clear","validateConfig","validateLLMConfig","createError","handleHTTPError","response","errorMessage","statusText","errorBody","JSON","parse","errorObj","handleNetworkError","handleParsingError","createHeaders","additionalHeaders","getAuthHeaders","processSSEStream","requestId","reader","_a","body","getReader","decoder","TextDecoder","buffer","done","value","read","decode","stream","lines","split","pop","line","startsWith","isComplete","chunk","parseStreamChunk","parseError","releaseLock","makeRequest","options","timeout","defaultTimeout","controller","AbortController","timeoutId","setTimeout","abort","fetch","signal","clearTimeout","ok","networkError","generateRequestId","Math","random","toString","substr","logRequest","request","window","location","hostname","logResponse","Map","BaseLLMProvider","llmProviderManager","registerProvider","providers","getProvider","getRegisteredProviders","Array","from","keys","hasProvider","has","sendStreamingRequest","validation","activeRequests","aborted","delete","cancelRequest","cancelAllRequests","values","getActiveRequestCount","size","getActiveRequestIds","providerName","clearAllModelCaches","createRequest","prompt","getProviderStats","stats","allProviders","registered","dispose","openaiProvider","arguments","headers","createRequestBody","method","stringify","Authorization","messages","trim","push","max_tokens","top_p","frequency_penalty","presence_penalty","parsed","validateStreamChunk","choice","streamChunk","total_tokens","tokenCount","validateOpenAIResponse","trimmedLine","errorData","json","isArray","filter","includes","map","description","getModelDescription","contextLength","getModelContextLength","maxOutputTokens","getModelMaxOutputTokens","pricing","getModelPricing","sort","a","b","getModelPriority","input","output","modelId","baseValidation","validatedConfig","supportedModels","join","normalizedBaseUrl","replace","some","anthropicProvider","_b","_d","_c","completionChunk","validateAnthropicResponse","geminiProvider","contents","systemInstruction","generationConfig","candidate","validateGeminiResponse","urlWithKey","URL","authHeader","searchParams","supportedGenerationMethods","getModelDisplayName","length"],"mappings":"6OAKa,MAAAA,EAA0BC,EAAS,CAC9CC,GAAIC,IACJC,OAAQC,EAAU,yBAClBC,QAASC,IACTC,MAAOL,IACPM,QAASC,EAAQT,EAAS,CACxBU,MAAOJ,IACPK,MAAOX,EAAS,CACdY,QAASV,IAAWW,WACpBC,KAAMZ,IAAWW,aAEnBE,cAAeb,IAAWc,gBAIGhB,EAAS,CACxCiB,MAAOjB,EAAS,CACdkB,QAAShB,IACTiB,KAAMjB,IACNkB,MAAOlB,IAAWc,WAAWH,WAC7BQ,KAAMnB,IAAWc,WAAWH,eAKnB,MAAAS,EAA6BtB,EAAS,CACjDmB,KAAMI,EAAO,CAAC,gBAAiB,sBAAuB,sBAAuB,qBAAsB,gBAAiB,iBACpHL,QAASlB,EAAS,CAChBC,GAAIC,IACJiB,KAAMf,EAAU,WAChBU,KAAMV,EAAU,aAChBQ,QAASH,EAAQT,EAAS,CACxBmB,KAAMf,EAAU,QAChBoB,KAAMtB,OAERK,MAAOL,IACPuB,YAAavB,IAAWc,WAAWH,WACnCa,cAAexB,IAAWc,WAAWH,WACrCc,MAAO3B,EAAS,CACd4B,aAActB,IACduB,cAAevB,MACdO,aACFA,WACHF,MAAOX,EAAS,CACdmB,KAAMf,EAAU,cAAcS,WAC9BW,KAAMtB,IAAWW,WACjBY,YAAavB,IAAWc,WAAWH,WACnCa,cAAexB,IAAWc,WAAWH,aACpCA,WACHiB,cAAe9B,EAAS,CACtBmB,KAAMf,EAAU,QAChBoB,KAAMtB,MACLW,WACHH,MAAOJ,IAAWO,aAGgBb,EAAS,CAC3CmB,KAAMf,EAAU,SAChBa,MAAOjB,EAAS,CACdmB,KAAMjB,IACNgB,QAAShB,QAKA,MAAA6B,EAA0B/B,EAAS,CAC9CgC,WAAYvB,EAAQT,EAAS,CAC3BY,QAASZ,EAAS,CAChBiC,MAAOxB,EAAQT,EAAS,CACtBwB,KAAMtB,OAERY,KAAMZ,MAERgC,aAAchC,IAAWW,WACzBH,MAAOJ,IACP6B,cAAe1B,EAAQT,EAAS,CAC9BoC,SAAUlC,IACVmC,YAAanC,OACXW,cAENyB,eAAgBtC,EAAS,CACvBmC,cAAe1B,EAAQT,EAAS,CAC9BoC,SAAUlC,IACVmC,YAAanC,SAEdW,aAG4Bb,EAAS,CACxCiB,MAAOjB,EAAS,CACdqB,KAAMf,IACNY,QAAShB,IACTqC,OAAQrC,QAKC,MAAAsC,EAAkBxC,EAAS,CACtCyC,SAAUlB,EAAO,CAAC,SAAU,YAAa,WACzCmB,OAAQxC,IAAWyC,IAAI,EAAG,uBAC1BC,QAAS1C,IAAW2C,IAAI,oBACxBtC,MAAOL,IAAWyC,IAAI,EAAG,qBACzBG,YAAaxC,IAAWqC,IAAI,GAAGI,IAAI,GACnCC,UAAW1C,IAAWqC,IAAI,GAAGI,IAAI,MACjCE,KAAM3C,IAAWqC,IAAI,GAAGI,IAAI,GAC5BG,iBAAkB5C,IAAWqC,KAAM,GAAEI,IAAI,GACzCI,gBAAiB7C,IAAWqC,KAAM,GAAEI,IAAI,GACxCK,cAAelD,MAIJmD,EAAiBrD,EAAS,CACrCsD,QAAS7C,EAAQP,KACjBqD,OAAQvD,EAAS,CACfyC,SAAUlB,EAAO,CAAC,SAAU,YAAa,WAAWV,WACpDN,MAAOL,IAAWW,WAClBiC,YAAaxC,IAAWqC,IAAI,GAAGI,IAAI,GAAGlC,WACtCmC,UAAW1C,IAAWqC,IAAI,GAAGI,IAAI,MAAOlC,WACxCoC,KAAM3C,IAAWqC,IAAI,GAAGI,IAAI,GAAGlC,WAC/BqC,iBAAkB5C,IAAWqC,QAAQI,IAAI,GAAGlC,WAC5CsC,gBAAiB7C,IAAWqC,QAAQI,IAAI,GAAGlC,WAC3CuC,cAAelD,IAAWW,aAE5B2C,GAAIxD,EAAS,CACXyD,YAAanD,IAAWqC,IAAI,GAAGI,IAAI,GAAGlC,WACtC6C,SAAUC,IAAY9C,WACtB+C,WAAYtD,IAAWqC,IAAI,GAAGI,IAAI,KAAMlC,WACxCgD,qBAAsBF,IAAY9C,WAClCiD,MAAOvC,EAAO,CAAC,QAAS,OAAQ,WAAWV,eAKlCkD,EAA0B/D,EAAS,CAC9CmB,KAAMI,EAAO,CAAC,OAAQ,UAAW,UACjCyC,kBAAmBL,MAoEd,SAASM,EAAiBC,GACzB,MAAAC,EAASd,EAAee,UAAUF,GACxC,OAAIC,EAAOE,QACF,CAAEA,SAAS,EAAMH,KAAMC,EAAOD,MAGhC,CACLG,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,2BACToD,QAASH,EAAOlD,MAAMsD,QAG5B,CAEO,SAASC,EAA0BN,GAClC,MAAAC,EAASJ,EAAwBK,UAAUF,GACjD,OAAIC,EAAOE,QACF,CAAEA,SAAS,EAAMH,KAAMC,EAAOD,MAGhC,CACLG,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,6BACToD,QAASH,EAAOlD,MAAMsD,QAG5B,CC7NO,MAAME,UAAyBC,MAMpC,WAAAC,CAAY1D,GACV2D,MAAM3D,EAAMC,SANE2D,EAAAC,KAAA,QACAD,EAAAC,KAAA,aACAD,EAAAC,KAAA,cACAD,EAAAC,KAAA,WAIdA,KAAKC,KAAO,mBACZD,KAAKzD,KAAOJ,EAAMI,KAClByD,KAAKE,UAAY/D,EAAM+D,eACE,IAArB/D,EAAMgE,aACRH,KAAKG,WAAahE,EAAMgE,iBAEJ,IAAlBhE,EAAMqD,UACRQ,KAAKR,QAAUrD,EAAMqD,QAEzB,EAMK,MAAeY,EAAf,MAAeA,EAAf,WAAAP,GAG+BE,EAAAC,KAAA,iBAAA,KACJD,EAAAC,KAAA,aAAA,GAIfD,EAAAC,KAAA,eAAe,IAAU,CAU1C,wBAAaK,CAAmBzC,EAAgBE,GACxC,MAAAwC,EAAW,GAAGN,KAAKC,QAAQrC,EAAO2C,OAAQ,MAAKzC,GAAW,YAC1D0C,EAASJ,EAAgBK,WAAWC,IAAIJ,GAG9C,GAAIE,GAAUG,KAAKC,MAAQJ,EAAOK,UAAYb,KAAKc,aAC1C,MAAA,CACLC,OAAQP,EAAOO,OACfP,QAAQ,EACRK,UAAWL,EAAOK,WAIlB,IACF,MAAME,QAAef,KAAKgB,qBAAqBpD,EAAQE,GAQhD,OALSsC,EAAAK,WAAWQ,IAAIX,EAAU,CACvCS,SACAF,UAAWF,KAAKC,QAGX,CACLG,SACAP,QAAQ,EACRK,UAAWF,KAAKC,aAEXzE,GAEP,OAAIqE,EAEK,CACLO,OAAQP,EAAOO,OACfP,QAAQ,EACRK,UAAWL,EAAOK,WAMf,CACLE,OAAQf,KAAKkB,oBACbV,QAAQ,EACRK,UAAWF,KAAKC,MAEpB,CACF,CAeO,eAAAO,GACLf,EAAgBK,WAAWW,OAC7B,CAKO,cAAAC,CAAe5C,GACpB,ODiEG,SAA2BW,GAC1B,MAAAC,EAAS3B,EAAgB4B,UAAUF,GACzC,OAAIC,EAAOE,QACF,CAAEA,SAAS,EAAMH,KAAMC,EAAOD,MAGhC,CACLG,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,4BACToD,QAASH,EAAOlD,MAAMsD,QAG5B,CC/EW6B,CAAkB7C,EAC3B,CAKU,WAAA8C,CACRhF,EACAH,EACA8D,GAAqB,EACrBC,EACAX,GAUO,MARiB,CACtBjD,OACAH,UACA8D,oBACmB,IAAfC,GAA4B,CAAEA,sBAClB,IAAZX,GAAyB,CAAEA,WAInC,CAKA,qBAAgBgC,CAAgBC,GAC9B,IACIjC,EADAkC,EAAe,QAAQD,EAAShE,WAAWgE,EAASE,aAGpD,IACI,MAAAC,QAAkBH,EAAS/E,OACjC,GAAIkF,EACE,IAGF,GAFUpC,EAAAqC,KAAKC,MAAMF,GAEE,iBAAZpC,GAAoC,OAAZA,EAAkB,CACnD,MAAMuC,EAAWvC,EACjB,GAAIuC,EAAgB,OAAkC,iBAAtBA,EAAgB,MAAgB,CACxD,MAAA5F,EAAQ4F,EAAgB,MACE,iBAArB5F,EAAe,UACxBuF,EAAevF,EAAe,QAEvB,KAA+B,iBAAxB4F,EAAkB,UAClCL,EAAeK,EAAkB,QAErC,CAAA,CACM,MAEIvC,EAAAoC,CACZ,CACF,CACM,MAER,CAEA,MAAM1B,EAAYuB,EAAShE,QAAU,KAA2B,MAApBgE,EAAShE,OAErD,OAAOuC,KAAKuB,YACV,QAAQE,EAAShE,SACjBiE,EACAxB,EACAuB,EAAShE,OACT+B,EAEJ,CAKU,kBAAAwC,CAAmB7F,GAC3B,MAAMC,EAAUD,aAAiByD,MAAQzD,EAAMC,QAAU,wBACzD,OAAO4D,KAAKuB,YAAY,gBAAiBnF,GAAS,OAAM,EAAWD,EACrE,CAKU,kBAAA8F,CAAmB9F,EAAgBiD,GAC3C,MAAMhD,EAAUD,aAAiByD,MAAQzD,EAAMC,QAAU,2BAClD,OAAA4D,KAAKuB,YAAY,gBAAiBnF,GAAS,OAAO,EAAW,CAAED,QAAOiD,QAC/E,CAKU,aAAA8C,CAActE,EAAgBuE,EAA4C,IAC3E,MAAA,CACL,eAAgB,mBAChB,aAAc,uBACXA,KACAnC,KAAKoC,eAAexE,GAE3B,CAyBA,sBAAiByE,CACfZ,EACAa,SAEM,MAAAC,EAAS,OAAAC,EAASf,EAAAgB,WAAM,EAAAD,EAAAE,YAC9B,IAAKH,EACG,MAAA,IAAI5C,EAAiBK,KAAKuB,YAC9B,mBACA,6BACA,IAIE,MAAAoB,EAAU,IAAIC,YACpB,IAAIC,EAAS,GAET,IACF,OAAa,CACX,MAAMC,KAAEA,EAAMC,MAAAA,SAAgBR,EAAOS,OACjC,GAAAF,EAAM,MAEVD,GAAUF,EAAQM,OAAOF,EAAO,CAAEG,QAAQ,IACpC,MAAAC,EAAQN,EAAOO,MAAM,MAClBP,EAAAM,EAAME,OAAS,GAExB,IAAA,MAAWC,KAAQH,EACb,GAAAG,EAAKC,WAAW,UAAW,CACvB,MAAAnE,EAAOkE,EAAK/C,MAAM,GACxB,GAAa,WAATnB,EAMF,iBALM,CACJkD,YACAxG,QAAS,GACT0H,YAAY,IAKZ,IACF,MAAMC,EAAQzD,KAAK0D,iBAAiBtE,EAAMkD,GACtCmB,UACIA,SAEDE,GAEP,QACF,CACF,CAEJ,CAAA,CACA,QACApB,EAAOqB,aACT,CACF,CAKA,iBAAgBC,CACd9F,EACA+F,EACAC,EAAkB/D,KAAKgE,gBAEjB,MAAAC,EAAa,IAAIC,gBACjBC,EAAYC,WAAW,IAAMH,EAAWI,QAASN,GAEnD,IACI,MAAAtC,QAAiB6C,MAAMvG,EAAK,IAC7B+F,EACHS,OAAQN,EAAWM,SAKjB,GAFJC,aAAaL,IAER1C,EAASgD,GAAI,CAChB,MAAMtI,QAAc6D,KAAKwB,gBAAgBC,GACnC,MAAA,IAAI9B,EAAiBxD,EAC7B,CAEO,OAAAsF,QACAtF,GAGP,GAFAqI,aAAaL,GAEThI,aAAiBwD,EACb,MAAAxD,EAGR,GAAIA,aAAiByD,OAAwB,eAAfzD,EAAM8D,KAC5B,MAAA,IAAIN,EAAiBK,KAAKuB,YAC9B,gBACA,2BAA2BwC,OAC3B,IAIE,MAAAW,EAAe1E,KAAKgC,mBAAmB7F,GACvC,MAAA,IAAIwD,EAAiB+E,EAC7B,CACF,CAKU,iBAAAC,GACR,MAAO,GAAG3E,KAAKC,QAAQU,KAAKC,SAASgE,KAAKC,SAASC,SAAS,IAAIC,OAAO,EAAG,IAC5E,CAKU,UAAAC,CAAWC,GAEG,oBAAXC,QAA0BA,OAAOC,SAASC,QAQvD,CAKU,WAAAC,CAAY5B,GAEE,oBAAXyB,QAAuD,cAA7BA,OAAOC,SAASC,UAA4B3B,EAAMD,UAMzF,GA3UAzD,EAPoBK,EAOL,aAAa,IAAIkF,KAP3B,IAAeC,EAAfnF,ECiOM,MAAAoF,EAAqB,IA1P3B,MAAA,WAAA3F,GACYE,EAAAC,KAAA,gBAAgBsF,KAChBvF,EAAAC,KAAA,qBAAqBsF,KAK/B,gBAAAG,CAAiB9H,GACtBqC,KAAK0F,UAAUzE,IAAItD,EAASsC,KAAMtC,EACpC,CAKO,WAAAgI,CAAY1F,GACjB,OAAOD,KAAK0F,UAAUhF,IAAIT,IAAS,IACrC,CAKO,sBAAA2F,GACL,OAAOC,MAAMC,KAAK9F,KAAK0F,UAAUK,OACnC,CAKO,WAAAC,CAAY/F,GACV,OAAAD,KAAK0F,UAAUO,IAAIhG,EAC5B,CAKA,0BAAciG,CAAqBjB,GACjC,MAAMtH,EAAWqC,KAAK2F,YAAYV,EAAQxG,OAAOd,UACjD,IAAKA,EACH,MAAM,IAAIgC,EAAiB,CACzBpD,KAAM,qBACNH,QAAS,aAAa6I,EAAQxG,OAAOd,8BACrCuC,WAAW,IAKf,MAAMiG,EAAaxI,EAAS0D,eAAe4D,EAAQxG,QAC/C,IAAC0H,EAAW5G,QACd,MAAM,IAAII,EAAiB,CACzBpD,KAAM,iBACNH,QAAS,0BAA0B+J,EAAWhK,MAAMC,UACpD8D,WAAW,EACXV,QAAS2G,EAAWhK,MAAMqD,UAKxB,MAAAyE,EAAa,IAAIC,gBACvBlE,KAAKoG,eAAenF,IAAIgE,EAAQ9J,GAAI8I,GAEhC,IAEF,UAAA,MAAiBR,KAAS9F,EAASuI,qBAAqBjB,GAAU,CAE5D,GAAAhB,EAAWM,OAAO8B,QAMpB,iBALM,CACJ/D,UAAW2C,EAAQ9J,GACnBW,QAAS,GACT0H,YAAY,UAKVC,CACR,CAAA,CACA,QAEKzD,KAAAoG,eAAeE,OAAOrB,EAAQ9J,GACrC,CACF,CAKO,aAAAoL,CAAcjE,GACnB,MAAM2B,EAAajE,KAAKoG,eAAe1F,IAAI4B,GAC3C,QAAI2B,IACFA,EAAWI,QACNrE,KAAAoG,eAAeE,OAAOhE,IACpB,EAGX,CAKO,iBAAAkE,GACL,IAAA,MAAWvC,KAAcjE,KAAKoG,eAAeK,SAC3CxC,EAAWI,QAEbrE,KAAKoG,eAAehF,OACtB,CAKO,qBAAAsF,GACL,OAAO1G,KAAKoG,eAAeO,IAC7B,CAKO,mBAAAC,GACL,OAAOf,MAAMC,KAAK9F,KAAKoG,eAAeL,OACxC,CAKA,wBAAa1F,CACXwG,EACAjJ,EACAE,GAEM,MAAAH,EAAWqC,KAAK2F,YAAYkB,GAClC,IAAKlJ,EACH,MAAM,IAAIgC,EAAiB,CACzBpD,KAAM,qBACNH,QAAS,aAAayK,uBACtB3G,WAAW,IAIR,OAAAvC,EAAS0C,mBAAmBzC,EAAQE,EAC7C,CAKO,eAAAqD,CAAgB0F,GACf,MAAAlJ,EAAWqC,KAAK2F,YAAYkB,GAC9BlJ,GAAY,oBAAqBA,GAClCA,EAAiBwD,iBAEtB,CAKO,mBAAA2F,GACL,IAAA,MAAWnJ,KAAYqC,KAAK0F,UAAUe,SAChC,oBAAqB9I,GACtBA,EAAiBwD,iBAGxB,CAKO,cAAAE,CAAewF,EAA2BpI,GACzC,MAAAd,EAAWqC,KAAK2F,YAAYkB,GAClC,OAAKlJ,EAUEA,EAAS0D,eAAe5C,GATtB,CACLc,SAAS,EACTpD,MAAO,CACLI,KAAM,qBACNH,QAAS,aAAayK,wBAM9B,CAKO,aAAAE,CACLC,EACAvI,EACAtD,GAGA,MAAMgL,EAAanG,KAAKqB,eAAe5C,EAAOd,SAAUc,GACpD,IAAC0H,EAAW5G,QACP,OAAA4G,EAUF,MAAA,CACL5G,SAAS,EACTH,KAT0B,CAC1BjE,GAAIA,GAAM6E,KAAK2E,kBAAkBlG,EAAOd,UACxCqJ,SACAvI,OAAQ0H,EAAW/G,KACnByB,UAAWF,KAAKC,OAOpB,CAKQ,iBAAA+D,CAAkBhH,GACxB,MAAO,GAAGA,KAAYgD,KAAKC,SAASgE,KAAKC,SAASC,SAAS,IAAIC,OAAO,EAAG,IAC3E,CAKO,gBAAAkC,GAIL,MAAMC,EAAyE,CAAA,EAGzEC,EAA8B,CAAC,SAAU,YAAa,UAC5D,IAAA,MAAWxJ,KAAYwJ,EACrBD,EAAMvJ,GAAY,CAChByJ,WAAYpH,KAAKgG,YAAYrI,GAC7ByI,eAAgB,GAKpB,IAAA,MAAW9D,KAAatC,KAAKoG,eAAeL,OAAQ,CAClD,MAAMpI,EAAW2E,EAAUc,MAAM,KAAK,GAClC8D,EAAMvJ,IACRuJ,EAAMvJ,GAAUyI,gBAEpB,CAEO,OAAAc,CACT,CAKO,OAAAG,GACLrH,KAAKwG,oBACLxG,KAAK0F,UAAUtE,OACjB,GC0MW,MAAAkG,EAAiB,IAvcvB,cAA6B/B,EAA7B,WAAA1F,GAAAC,SAAAyH,WACkBxH,EAAAC,KAAA,OAAA,SAAA,CAKvB,0BAAckG,CAAqBjB,GACjCjF,KAAKgF,WAAWC,GAEhB,MAAMlH,EAAM,GAAGkH,EAAQxG,OAAOX,2BACxB0J,EAAUxH,KAAKkC,cAAc+C,EAAQxG,OAAOb,QAC5C6E,EAAOzC,KAAKyH,kBAAkBxC,GAEhC,IACF,MAAMxD,QAAiBzB,KAAK6D,YAAY9F,EAAK,CAC3C2J,OAAQ,OACRF,UACA/E,KAAMZ,KAAK8F,UAAUlF,WAGhBzC,KAAKqC,iBAAiBZ,EAAUwD,EAAQ9J,UACxCgB,GACP,GAAIA,aAAiBwD,EACb,MAAAxD,EAGF,MAAAuI,EAAe1E,KAAKgC,mBAAmB7F,GACvC,MAAA,IAAIwD,EAAiB+E,EAC7B,CACF,CAKU,cAAAtC,CAAexE,GAChB,MAAA,CACLgK,cAAiB,UAAUhK,IAE/B,CAKU,iBAAA6J,CAAkBxC,GAC1B,MAAM4C,EAAqD,GAgBpD,OAbH5C,EAAQxG,OAAOH,cAAcwJ,QAC/BD,EAASE,KAAK,CACZ/L,KAAM,SACNF,QAASmJ,EAAQxG,OAAOH,gBAK5BuJ,EAASE,KAAK,CACZ/L,KAAM,OACNF,QAASmJ,EAAQ+B,SAGZ,CACLvL,MAAOwJ,EAAQxG,OAAOhD,MACtBoM,WACA7J,YAAaiH,EAAQxG,OAAOT,YAC5BgK,WAAY/C,EAAQxG,OAAOP,UAC3B+J,MAAOhD,EAAQxG,OAAON,KACtB+J,kBAAmBjD,EAAQxG,OAAOL,iBAClC+J,iBAAkBlD,EAAQxG,OAAOJ,gBACjC6E,QAAQ,EAEZ,CAKU,gBAAAQ,CAAiBtE,EAAckD,SACnC,IACI,MAAA8F,EAASvG,KAAKC,MAAM1C,GACpB+G,EAAanG,KAAKqI,oBAAoBD,GAExC,IAACjC,EAAW5G,QAEP,OAAA,KAGT,MACM+I,EADQnC,EAAW/G,KACJ1D,QAAQ,GAE7B,IAAK4M,EACI,OAAA,KAGH,MAAAxM,EAAUwM,EAAOzM,MAAMC,SAAW,GAClC0H,EAAsC,OAAzB8E,EAAOrM,cAEpBsM,EAA8B,CAClCjG,YACAxG,UACA0H,gBACIA,IAAe,OAAAhB,EAAe4F,EAAAvL,gBAAO2L,eAAgB,CACvDC,WAAaL,EAAevL,MAAM2L,eAK/B,OADPxI,KAAKqF,YAAYkD,GACVA,QACApM,GAEA,OAAA,IACT,CACF,CAKU,mBAAAkM,CAAoBjJ,GAC5B,OHoBG,SAAgCA,GAC/B,MAAAC,EAASpE,EAAwBqE,UAAUF,GACjD,OAAIC,EAAOE,QACF,CAAEA,SAAS,EAAMH,KAAMC,EAAOD,MAGhC,CACLG,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,iCACToD,QAASH,EAAOlD,MAAMsD,QAG5B,CGlCWiJ,CAAuBtJ,EAChC,CAKA,sBAA0BiD,CACxBZ,EACAa,SAEM,MAAAC,EAAS,OAAAC,EAASf,EAAAgB,WAAM,EAAAD,EAAAE,YAC9B,IAAKH,EACG,MAAA,IAAI5C,EAAiBK,KAAKuB,YAC9B,mBACA,yCACA,IAIE,MAAAoB,EAAU,IAAIC,YACpB,IAAIC,EAAS,GAET,IACF,OAAa,CACX,MAAMC,KAAEA,EAAMC,MAAAA,SAAgBR,EAAOS,OACjC,GAAAF,EAAM,MAEVD,GAAUF,EAAQM,OAAOF,EAAO,CAAEG,QAAQ,IACpC,MAAAC,EAAQN,EAAOO,MAAM,MAClBP,EAAAM,EAAME,OAAS,GAExB,IAAA,MAAWC,KAAQH,EAAO,CAClB,MAAAwF,EAAcrF,EAAKwE,OAErB,GAAAa,EAAYpF,WAAW,UAAW,CAC9B,MAAAnE,EAAOuJ,EAAYpI,MAAM,GAG/B,GAAa,WAATnB,EAMF,iBALM,CACJkD,YACAxG,QAAS,GACT0H,YAAY,IAMhB,MAAMC,EAAQzD,KAAK0D,iBAAiBtE,EAAMkD,GAC1C,GAAImB,UACIA,EAGFA,EAAMD,YACR,MAGN,CACF,CACF,QACOrH,GACD,MAAAwH,EAAa3D,KAAKiC,mBAAmB9F,GACrC,MAAA,IAAIwD,EAAiBgE,EAAU,CACrC,QACApB,EAAOqB,aACT,CACF,CAKA,qBAAyBpC,CAAgBC,SACvC,IACIjC,EADAkC,EAAe,qBAAqBD,EAAShE,UAAUgE,EAASE,aAGhE,IACI,MAAAC,QAAkBH,EAAS/E,OACjC,GAAIkF,EACE,IACI,MAAAgH,EAAY/G,KAAKC,MAAMF,GACnBpC,EAAAoJ,GAGN,OAAApG,EAAAoG,EAAUzM,YAAV,EAAAqG,EAAiBpG,WACnBsF,EAAekH,EAAUzM,MAAMC,QACjC,CACM,MACIoD,EAAAoC,CACZ,CACF,CACM,MAER,CAGM,MAAA1B,EAAYuB,EAAShE,QAAU,KACA,MAApBgE,EAAShE,QACW,MAApBgE,EAAShE,OAE1B,OAAOuC,KAAKuB,YACV,eAAeE,EAAShE,SACxBiE,EACAxB,EACAuB,EAAShE,OACT+B,EAEJ,CAKA,0BAAgBwB,CAAqBpD,EAAgBE,GAC7C,MAAAC,EAAM,GAAGD,GAAW,qCACpB0J,EAAUxH,KAAKkC,cAActE,GAE/B,IACF,MAAM6D,QAAiBzB,KAAK6D,YAAY9F,EAAK,CAC3C2J,OAAQ,MACRF,YAGIpI,QAAaqC,EAASoH,OAExB,IAACzJ,EAAKA,OAASyG,MAAMiD,QAAQ1J,EAAKA,MAC9B,MAAA,IAAIQ,MAAM,kDAgCX,OA5BqBR,EAAKA,KAC9B2J,OAAQtN,GAEAA,EAAMN,KACXM,EAAMN,GAAGoI,WAAW,SACpB9H,EAAMN,GAAG6N,SAAS,SAClBvN,EAAMN,GAAG6N,SAAS,WAGrBC,IAAKxN,IAAgB,CACpBN,GAAIM,EAAMN,GACV8E,KAAMxE,EAAMN,GACZ+N,YAAalJ,KAAKmJ,oBAAoB1N,EAAMN,IAC5CiO,cAAepJ,KAAKqJ,sBAAsB5N,EAAMN,IAChDmO,gBAAiBtJ,KAAKuJ,wBAAwB9N,EAAMN,IACpDqO,QAASxJ,KAAKyJ,gBAAgBhO,EAAMN,OAErCuO,KAAK,CAACC,EAAcC,KAEb,MAAAC,EAAoB1O,GACpBA,EAAG6N,SAAS,UAAkB,EAC9B7N,EAAG6N,SAAS,SAAiB,EAC7B7N,EAAG6N,SAAS,WAAmB,EAC5B,EAET,OAAOa,EAAiBF,EAAExO,IAAM0O,EAAiBD,EAAEzO,YAIhDgB,GAED,MAAAA,CACR,CACF,CAKU,iBAAA+E,GACD,MAAA,CACL,CACE/F,GAAI,SACJ8E,KAAM,SACNiJ,YAAa,iCACbE,cAAe,MACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,EAAKC,OAAQ,KAEjC,CACE5O,GAAI,cACJ8E,KAAM,cACNiJ,YAAa,yCACbE,cAAe,MACfE,gBAAiB,MACjBE,QAAS,CAAEM,MAAO,IAAMC,OAAQ,KAElC,CACE5O,GAAI,cACJ8E,KAAM,cACNiJ,YAAa,qCACbE,cAAe,MACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,GAAMC,OAAQ,KAElC,CACE5O,GAAI,QACJ8E,KAAM,QACNiJ,YAAa,uBACbE,cAAe,KACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,GAAMC,OAAQ,KAElC,CACE5O,GAAI,gBACJ8E,KAAM,gBACNiJ,YAAa,4BACbE,cAAe,MACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,GAAKC,OAAQ,MAGrC,CAKQ,mBAAAZ,CAAoBa,GAQnB,MAPsC,CAC3C,SAAU,iCACV,cAAe,yCACf,cAAe,qCACf,QAAS,uBACT,gBAAiB,6BAECA,IAAY,uBAClC,CAKQ,qBAAAX,CAAsBW,GAQrB,MAPwC,CAC7C,SAAU,MACV,cAAe,MACf,cAAe,MACf,QAAS,KACT,gBAAiB,OAEGA,IAAY,IACpC,CAKQ,uBAAAT,CAAwBS,GAQvB,MAPyC,CAC9C,SAAU,KACV,cAAe,MACf,cAAe,KACf,QAAS,KACT,gBAAiB,MAEIA,IAAY,IACrC,CAKQ,eAAAP,CAAgBO,GAQtB,MAPmE,CACjE,SAAU,CAAEF,MAAO,EAAKC,OAAQ,IAChC,cAAe,CAAED,MAAO,IAAMC,OAAQ,IACtC,cAAe,CAAED,MAAO,GAAMC,OAAQ,IACtC,QAAS,CAAED,MAAO,GAAMC,OAAQ,IAChC,gBAAiB,CAAED,MAAO,GAAKC,OAAQ,MAE1BC,IAAY,CAAEF,MAAO,EAAKC,OAAQ,EACnD,CAKgB,cAAA1I,CAAe5C,GAEvB,MAAAwL,EAAiBnK,MAAMuB,eAAe5C,GACxC,IAACwL,EAAe1K,QACX,OAAA0K,EAGT,MAAMC,EAAkBD,EAAe7K,KAGnC,GAA6B,WAA7B8K,EAAgBvM,SACX,MAAA,CACL4B,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,kDAMf,IAAK8N,EAAgBtM,OAAO2F,WAAW,OAC9B,MAAA,CACLhE,SAAS,EACTpD,MAAO,CACLI,KAAM,kBACNH,QAAS,yCAMf,MAAM+N,EAAkB,CACtB,SACA,cACA,cACA,QACA,iBAGF,IAAKA,EAAgBnB,SAASkB,EAAgBzO,OACrC,MAAA,CACL8D,SAAS,EACTpD,MAAO,CACLI,KAAM,oBACNH,QAAS,UAAU8N,EAAgBzO,8CAA8C0O,EAAgBC,KAAK,UAM5G,MAKMC,EAAoBH,EAAgBpM,QAAQwM,QAAQ,MAAO,IAK1D,MAVe,CACpB,4BACA,8BAIiBC,KAAYxM,GAAAA,EAAIuM,QAAQ,MAAO,MAAQD,GAInD,CACL9K,SAAS,EACTH,KAAM8K,EAEV,GC3CW,MAAAM,EAAoB,IAxZ1B,cAAgCjF,EAAhC,WAAA1F,GAAAC,SAAAyH,WACkBxH,EAAAC,KAAA,OAAA,YAAA,CAKvB,0BAAckG,CAAqBjB,GACjCjF,KAAKgF,WAAWC,GAEhB,MAAMlH,EAAM,GAAGkH,EAAQxG,OAAOX,sBACxB0J,EAAUxH,KAAKkC,cAAc+C,EAAQxG,OAAOb,OAAQ,CACxD,oBAAqB,eAEjB6E,EAAOzC,KAAKyH,kBAAkBxC,GAEhC,IACF,MAAMxD,QAAiBzB,KAAK6D,YAAY9F,EAAK,CAC3C2J,OAAQ,OACRF,UACA/E,KAAMZ,KAAK8F,UAAUlF,WAGhBzC,KAAKqC,iBAAiBZ,EAAUwD,EAAQ9J,UACxCgB,GACP,GAAIA,aAAiBwD,EACb,MAAAxD,EAGF,MAAAuI,EAAe1E,KAAKgC,mBAAmB7F,GACvC,MAAA,IAAIwD,EAAiB+E,EAC7B,CACF,CAKU,cAAAtC,CAAexE,GAChB,MAAA,CACL,YAAaA,EAEjB,CAKU,iBAAA6J,CAAkBxC,GAC1B,MAAM4C,EAAqD,GAG3DA,EAASE,KAAK,CACZ/L,KAAM,OACNF,QAASmJ,EAAQ+B,SAGnB,MAAMvE,EAAgC,CACpChH,MAAOwJ,EAAQxG,OAAOhD,MACtBoM,WACAG,WAAY/C,EAAQxG,OAAOP,UAC3BF,YAAaiH,EAAQxG,OAAOT,YAC5BiK,MAAOhD,EAAQxG,OAAON,KACtB+E,QAAQ,GAQH,OAJH+B,EAAQxG,OAAOH,cAAcwJ,SAC1BrF,EAAQ,OAAIwC,EAAQxG,OAAOH,eAG3BmE,CACT,CAKU,gBAAAiB,CAAiBtE,EAAckD,eACnC,IACI,MAAA8F,EAASvG,KAAKC,MAAM1C,GACpB+G,EAAanG,KAAKqI,oBAAoBD,GAExC,IAACjC,EAAW5G,QAEP,OAAA,KAGT,MAAMkE,EAAQ0C,EAAW/G,KAGzB,OAAQqE,EAAMpH,MACZ,IAAK,gBACI,MAAA,CACLiG,YACAxG,QAAS,GACT0H,YAAY,GAGhB,IAAK,sBACC,OAAA,OAAAhB,EAAAiB,EAAM5H,YAAN,EAAA2G,EAAa9F,MACR,CACL4F,YACAxG,QAAS2H,EAAM5H,MAAMa,KACrB8G,YAAY,GAGT,KAET,IAAK,gBACC,GAAA,OAAAiH,EAAAhH,EAAM5H,YAAN,EAAA4O,EAAa9N,YAAa,CAC5B,MAAM4L,EAA8B,CAClCjG,YACAxG,QAAS,GACT0H,YAAY,GAIV,OAAA,OAAAkH,EAAA,OAAMC,EAAAlH,EAAArH,cAAS,EAAAuO,EAAA9N,gBAAOE,eACjB,IACFwL,EACHE,WAAYhF,EAAMrH,QAAQS,MAAMC,aAAe2G,EAAMrH,QAAQS,MAAME,gBAIvEiD,KAAKqF,YAAYkD,GACVA,EACT,CACO,OAAA,KAET,IAAK,eAAgB,CACnB,MAAMqC,EAAkC,CACtCtI,YACAxG,QAAS,GACT0H,YAAY,GAIP,OADPxD,KAAKqF,YAAYuF,GACVA,CACT,CAEA,QACS,OAAA,YAEJzO,GAEA,OAAA,IACT,CACF,CAKU,mBAAAkM,CAAoBjJ,GAC5B,OJEG,SAAmCA,GAClC,MAAAC,EAAS7C,EAA2B8C,UAAUF,GACpD,OAAIC,EAAOE,QACF,CAAEA,SAAS,EAAMH,KAAMC,EAAOD,MAGhC,CACLG,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,oCACToD,QAASH,EAAOlD,MAAMsD,QAG5B,CIhBWoL,CAA0BzL,EACnC,CAKA,sBAA0BiD,CACxBZ,EACAa,SAEM,MAAAC,EAAS,OAAAC,EAASf,EAAAgB,WAAM,EAAAD,EAAAE,YAC9B,IAAKH,EACG,MAAA,IAAI5C,EAAiBK,KAAKuB,YAC9B,mBACA,4CACA,IAIE,MAAAoB,EAAU,IAAIC,YACpB,IAAIC,EAAS,GAET,IACF,OAAa,CACX,MAAMC,KAAEA,EAAMC,MAAAA,SAAgBR,EAAOS,OACjC,GAAAF,EAAM,MAEVD,GAAUF,EAAQM,OAAOF,EAAO,CAAEG,QAAQ,IACpC,MAAAC,EAAQN,EAAOO,MAAM,MAClBP,EAAAM,EAAME,OAAS,GAExB,IAAA,MAAWC,KAAQH,EAAO,CAClB,MAAAwF,EAAcrF,EAAKwE,OAErB,GAAAa,EAAYpF,WAAW,UAAW,CAC9B,MAAAnE,EAAOuJ,EAAYpI,MAAM,GAG3B,IAACnB,GAAiB,OAATA,EACX,SAIF,MAAMqE,EAAQzD,KAAK0D,iBAAiBtE,EAAMkD,GAC1C,GAAImB,UACIA,EAGFA,EAAMD,YACR,MAGK,MAAA,GAAAmF,EAAYpF,WAAW,WAAY,CAG5C,GAAkB,iBADAoF,EAAYpI,MAAM,GAOlC,iBALM,CACJ+B,YACAxG,QAAS,GACT0H,YAAY,GAIlB,CACF,CACF,QACOrH,GACD,MAAAwH,EAAa3D,KAAKiC,mBAAmB9F,GACrC,MAAA,IAAIwD,EAAiBgE,EAAU,CACrC,QACApB,EAAOqB,aACT,CACF,CAKA,qBAAyBpC,CAAgBC,SACvC,IACIjC,EADAkC,EAAe,wBAAwBD,EAAShE,UAAUgE,EAASE,aAGnE,IACI,MAAAC,QAAkBH,EAAS/E,OACjC,GAAIkF,EACE,IACI,MAAAgH,EAAY/G,KAAKC,MAAMF,GACnBpC,EAAAoJ,GAGN,OAAApG,EAAAoG,EAAUzM,YAAV,EAAAqG,EAAiBpG,SACnBsF,EAAekH,EAAUzM,MAAMC,QACtBwM,EAAUxM,UACnBsF,EAAekH,EAAUxM,QAC3B,CACM,MACIoD,EAAAoC,CACZ,CACF,CACM,MAER,CAGM,MAAA1B,EAAYuB,EAAShE,QAAU,KACA,MAApBgE,EAAShE,QACW,MAApBgE,EAAShE,OAE1B,OAAOuC,KAAKuB,YACV,kBAAkBE,EAAShE,SAC3BiE,EACAxB,EACAuB,EAAShE,OACT+B,EAEJ,CAMA,0BAAgBwB,GAId,OAAOhB,KAAKkB,mBACd,CAKU,iBAAAA,GACD,MAAA,CACL,CACE/F,GAAI,6BACJ8E,KAAM,oBACNiJ,YAAa,kDACbE,cAAe,IACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,EAAKC,OAAQ,KAEjC,CACE5O,GAAI,4BACJ8E,KAAM,mBACNiJ,YAAa,mCACbE,cAAe,IACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,GAAKC,OAAQ,IAEjC,CACE5O,GAAI,yBACJ8E,KAAM,gBACNiJ,YAAa,0CACbE,cAAe,IACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,GAAMC,OAAQ,KAElC,CACE5O,GAAI,2BACJ8E,KAAM,kBACNiJ,YAAa,oCACbE,cAAe,IACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,EAAKC,OAAQ,KAEjC,CACE5O,GAAI,0BACJ8E,KAAM,iBACNiJ,YAAa,gCACbE,cAAe,IACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,IAAMC,OAAQ,OAGtC,CAKgB,cAAA1I,CAAe5C,GAEvB,MAAAwL,EAAiBnK,MAAMuB,eAAe5C,GACxC,IAACwL,EAAe1K,QACX,OAAA0K,EAGT,MAAMC,EAAkBD,EAAe7K,KAGnC,GAA6B,cAA7B8K,EAAgBvM,SACX,MAAA,CACL4B,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,wDAMf,IAAK8N,EAAgBtM,OAAO2F,WAAW,WAC9B,MAAA,CACLhE,SAAS,EACTpD,MAAO,CACLI,KAAM,kBACNH,QAAS,gDAMf,MAAM+N,EAAkB,CACtB,6BACA,4BACA,yBACA,2BACA,2BAGF,IAAKA,EAAgBnB,SAASkB,EAAgBzO,OACrC,MAAA,CACL8D,SAAS,EACTpD,MAAO,CACLI,KAAM,oBACNH,QAAS,UAAU8N,EAAgBzO,8CAA8C0O,EAAgBC,KAAK,UAM5G,MAKMC,EAAoBH,EAAgBpM,QAAQwM,QAAQ,MAAO,IAM7D,MAXkB,CACpB,4BACA,8BAIiBC,KAAYxM,GAAAA,EAAIuM,QAAQ,MAAO,MAAQD,GAKtDH,EAAgBhM,UAAY,KACvB,CACLqB,SAAS,EACTpD,MAAO,CACLI,KAAM,qBACNH,QAAS,sDAKR,CACLmD,SAAS,EACTH,KAAM8K,EAEV,GCyGW,MAAAY,EAAiB,IA7fvB,cAA6BvF,EAA7B,WAAA1F,GAAAC,SAAAyH,WACkBxH,EAAAC,KAAA,OAAA,SAAA,CAKvB,0BAAckG,CAAqBjB,GACjCjF,KAAKgF,WAAWC,GAEV,MAAAlH,EAAM,GAAGkH,EAAQxG,OAAOX,kBAAkBmH,EAAQxG,OAAOhD,8BACzD+L,EAAUxH,KAAKkC,cAAc+C,EAAQxG,OAAOb,QAC5C6E,EAAOzC,KAAKyH,kBAAkBxC,GAEhC,IACF,MAAMxD,QAAiBzB,KAAK6D,YAAY9F,EAAK,CAC3C2J,OAAQ,OACRF,UACA/E,KAAMZ,KAAK8F,UAAUlF,WAGhBzC,KAAKqC,iBAAiBZ,EAAUwD,EAAQ9J,UACxCgB,GACP,GAAIA,aAAiBwD,EACb,MAAAxD,EAGF,MAAAuI,EAAe1E,KAAKgC,mBAAmB7F,GACvC,MAAA,IAAIwD,EAAiB+E,EAC7B,CACF,CAKU,cAAAtC,GAER,MAAO,EACT,CAKU,iBAAAqF,CAAkBxC,GAC1B,MAAM8F,EAAoE,GAGpEC,EAAoB/F,EAAQxG,OAAOH,cAAcwJ,OACnD,CAAE3K,MAAO,CAAC,CAAET,KAAMuI,EAAQxG,OAAOH,sBACjC,EAGJyM,EAAShD,KAAK,CACZ/L,KAAM,OACNmB,MAAO,CAAC,CAAET,KAAMuI,EAAQ+B,WAG1B,MAAMvE,EAAgC,CACpCsI,WACAE,iBAAkB,CAChBjN,YAAaiH,EAAQxG,OAAOT,YAC5BsL,gBAAiBrE,EAAQxG,OAAOP,UAChCC,KAAM8G,EAAQxG,OAAON,OAQlB,OAJH6M,IACFvI,EAAwB,kBAAIuI,GAGvBvI,CACT,CAKU,gBAAAiB,CAAiBtE,EAAckD,eACnC,IACI,MAAA8F,EAASvG,KAAKC,MAAM1C,GACpB+G,EAAanG,KAAKqI,oBAAoBD,GAExC,IAACjC,EAAW5G,QAEP,OAAA,KAGT,MACM2L,EAAY,OAAA1I,EADJ2D,EAAW/G,KACDlC,iBAAa,EAAAsF,EAAA,GAErC,IAAK0I,EACI,OAAA,KAGH,MAAApP,GAAU,OAAA4O,EAAA,SAAA,WAAU5O,cAAV,EAAA2O,EAAmBtN,YAAQ,EAAAwN,EAAA,aAAIjO,OAAQ,GACjD8G,OAAwC,IAA3B0H,EAAU9N,aAEvBmL,EAA8B,CAClCjG,YACAxG,UACA0H,cAOK,OAJHA,GACFxD,KAAKqF,YAAYkD,GAGZA,QACApM,GAEA,OAAA,IACT,CACF,CAKU,mBAAAkM,CAAoBjJ,GAC5B,OLoDG,SAAgCA,GAC/B,MAAAC,EAASpC,EAAwBqC,UAAUF,GACjD,OAAIC,EAAOE,QACF,CAAEA,SAAS,EAAMH,KAAMC,EAAOD,MAGhC,CACLG,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,iCACToD,QAASH,EAAOlD,MAAMsD,QAG5B,CKlEW0L,CAAuB/L,EAChC,CAKA,sBAA0BiD,CACxBZ,EACAa,SAEM,MAAAC,EAAS,OAAAC,EAASf,EAAAgB,WAAM,EAAAD,EAAAE,YAC9B,IAAKH,EACG,MAAA,IAAI5C,EAAiBK,KAAKuB,YAC9B,mBACA,yCACA,IAIE,MAAAoB,EAAU,IAAIC,YACpB,IAAIC,EAAS,GAET,IACF,OAAa,CACX,MAAMC,KAAEA,EAAMC,MAAAA,SAAgBR,EAAOS,OACjC,GAAAF,EAAM,MAEVD,GAAUF,EAAQM,OAAOF,EAAO,CAAEG,QAAQ,IACpC,MAAAC,EAAQN,EAAOO,MAAM,MAClBP,EAAAM,EAAME,OAAS,GAExB,IAAA,MAAWC,KAAQH,EAAO,CAClB,MAAAwF,EAAcrF,EAAKwE,OAErB,GAAAa,EAAYpF,WAAW,UAAW,CAC9B,MAAAnE,EAAOuJ,EAAYpI,MAAM,GAG3B,IAACnB,GAAiB,OAATA,EACX,SAIF,MAAMqE,EAAQzD,KAAK0D,iBAAiBtE,EAAMkD,GAC1C,GAAImB,UACIA,EAGFA,EAAMD,YACR,MAGN,CACF,CACF,QACOrH,GACD,MAAAwH,EAAa3D,KAAKiC,mBAAmB9F,GACrC,MAAA,IAAIwD,EAAiBgE,EAAU,CACrC,QACApB,EAAOqB,aACT,CACF,CAKA,iBAAyBC,CACvB9F,EACA+F,EACAC,EAAkB/D,KAAKgE,gBAGjB,MAAAoH,EAAa,IAAIC,IAAItN,GAGrByJ,EAAU1D,EAAQ0D,SAAqC,GAC7D,IAAI5J,EAAS,GAGb,GAAIkG,EAAQrB,MAAgC,iBAAjBqB,EAAQrB,KAC7B,IAGI,MAAA6I,EAAa9D,EAAuB,eACtC,MAAA8D,OAAA,EAAAA,EAAY/H,WAAW,cAChB3F,EAAA0N,EAAW/K,MAAM,GAC5B,CACM,MAER,CASF,OANI3C,IACSwN,EAAAG,aAAatK,IAAI,MAAOrD,UAE5B4J,EAAuB,eAGzB1H,MAAM+D,YAAYuH,EAAWtG,WAAY,IAC3ChB,EACH0D,WACCzD,EACL,CAKmB,aAAA7B,CAActE,EAAgBuE,EAA4C,IACpF,MAAA,CACL,eAAgB,mBAChB,aAAc,oBACdyF,cAAiB,UAAUhK,OACxBuE,EAEP,CAKA,qBAAyBX,CAAgBC,SACvC,IACIjC,EADAkC,EAAe,qBAAqBD,EAAShE,UAAUgE,EAASE,aAGhE,IACI,MAAAC,QAAkBH,EAAS/E,OACjC,GAAIkF,EACE,IACI,MAAAgH,EAAY/G,KAAKC,MAAMF,GACnBpC,EAAAoJ,GAGN,OAAApG,EAAAoG,EAAUzM,YAAV,EAAAqG,EAAiBpG,SACnBsF,EAAekH,EAAUzM,MAAMC,QACtBwM,EAAUxM,UACnBsF,EAAekH,EAAUxM,QAC3B,CACM,MACIoD,EAAAoC,CACZ,CACF,CACM,MAER,CAGM,MAAA1B,EAAYuB,EAAShE,QAAU,KACA,MAApBgE,EAAShE,QACW,MAApBgE,EAAShE,OAE1B,OAAOuC,KAAKuB,YACV,eAAeE,EAAShE,SACxBiE,EACAxB,EACAuB,EAAShE,OACT+B,EAEJ,CAKA,0BAAgBwB,CAAqBpD,EAAgBE,GAC7C,MAAAC,EAAM,GAAGD,GAAW,4DAEtB,IACF,MAAM2D,QAAiB6C,MAAM,GAAGvG,SAAWH,IAAU,CACnD8J,OAAQ,MACRF,QAAS,CACP,eAAgB,mBAChB,aAAc,uBAId,IAAC/F,EAASgD,GACN,MAAA,IAAI7E,MAAM,QAAQ6B,EAAShE,WAAWgE,EAASE,cAGjD,MAAAvC,QAAaqC,EAASoH,OAExB,IAACzJ,EAAK2B,SAAW8E,MAAMiD,QAAQ1J,EAAK2B,QAChC,MAAA,IAAInB,MAAM,kDAiCX,OA7BqBR,EAAK2B,OAC9BgI,OAAQtN,UAEA,OAAAA,EAAMwE,OACN,OAAAuC,EAAA/G,EAAM+P,iCAAN,EAAAhJ,EAAkCwG,SAAS,sBAC1CvN,EAAMwE,KAAK+I,SAAS,eAE7BC,IAAKxN,IACJ,MAAMuO,EAAUvO,EAAMwE,KAAKqK,QAAQ,UAAW,IACvC,MAAA,CACLnP,GAAI6O,EACJ/J,KAAMD,KAAKyL,oBAAoBzB,GAC/Bd,YAAazN,EAAMyN,aAAelJ,KAAKmJ,oBAAoBa,GAC3DZ,cAAepJ,KAAKqJ,sBAAsBW,GAC1CV,gBAAiBtJ,KAAKuJ,wBAAwBS,GAC9CR,QAASxJ,KAAKyJ,gBAAgBO,MAGjCN,KAAK,CAACC,EAAcC,KAEb,MAAAC,EAAoB1O,GACpBA,EAAG6N,SAAS,kBAA0B,EACtC7N,EAAG6N,SAAS,oBAA4B,EACxC7N,EAAG6N,SAAS,kBAA0B,EACnC,EAET,OAAOa,EAAiBF,EAAExO,IAAM0O,EAAiBD,EAAEzO,YAIhDgB,GAED,MAAAA,CACR,CACF,CAKU,iBAAA+E,GACD,MAAA,CACL,CACE/F,GAAI,iBACJ8E,KAAM,iBACNiJ,YAAa,iDACbE,cAAe,IACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,KAAMC,OAAQ,IAElC,CACE5O,GAAI,mBACJ8E,KAAM,mBACNiJ,YAAa,8CACbE,cAAe,IACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,KAAOC,OAAQ,KAEnC,CACE5O,GAAI,iBACJ8E,KAAM,iBACNiJ,YAAa,4BACbE,cAAe,MACfE,gBAAiB,KACjBE,QAAS,CAAEM,MAAO,GAAKC,OAAQ,MAGrC,CAKQ,mBAAA0B,CAAoBzB,GAMnB,MALsC,CAC3C,iBAAkB,iBAClB,mBAAoB,mBACpB,iBAAkB,kBAEAA,IAAYA,CAClC,CAKQ,mBAAAb,CAAoBa,GAMnB,MALsC,CAC3C,iBAAkB,iDAClB,mBAAoB,8CACpB,iBAAkB,6BAEAA,IAAY,8BAClC,CAKQ,qBAAAX,CAAsBW,GAMrB,MALwC,CAC7C,iBAAkB,IAClB,mBAAoB,IACpB,iBAAkB,OAEEA,IAAY,KACpC,CAKQ,uBAAAT,CAAwBS,GAMvB,MALyC,CAC9C,iBAAkB,KAClB,mBAAoB,KACpB,iBAAkB,MAEGA,IAAY,IACrC,CAKQ,eAAAP,CAAgBO,GAMtB,MALmE,CACjE,iBAAkB,CAAEF,MAAO,KAAMC,OAAQ,GACzC,mBAAoB,CAAED,MAAO,KAAOC,OAAQ,IAC5C,iBAAkB,CAAED,MAAO,GAAKC,OAAQ,MAE3BC,IAAY,CAAEF,MAAO,GAAKC,OAAQ,IACnD,CAKgB,cAAA1I,CAAe5C,GAEvB,MAAAwL,EAAiBnK,MAAMuB,eAAe5C,GACxC,IAACwL,EAAe1K,QACX,OAAA0K,EAGT,MAAMC,EAAkBD,EAAe7K,KAGnC,GAA6B,WAA7B8K,EAAgBvM,SACX,MAAA,CACL4B,SAAS,EACTpD,MAAO,CACLI,KAAM,mBACNH,QAAS,kDAMX,GAAA8N,EAAgBtM,OAAO8N,OAAS,GAC3B,MAAA,CACLnM,SAAS,EACTpD,MAAO,CACLI,KAAM,kBACNH,QAAS,2CAMf,MAAM+N,EAAkB,CACtB,iBACA,mBACA,kBAGF,IAAKA,EAAgBnB,SAASkB,EAAgBzO,OACrC,MAAA,CACL8D,SAAS,EACTpD,MAAO,CACLI,KAAM,oBACNH,QAAS,UAAU8N,EAAgBzO,8CAA8C0O,EAAgBC,KAAK,UAM5G,MAKMC,EAAoBH,EAAgBpM,QAAQwM,QAAQ,MAAO,IAM7D,MAXkB,CACpB,mDACA,qDAIiBC,KAAYxM,GAAAA,EAAIuM,QAAQ,MAAO,MAAQD,GAKtDH,EAAgBhM,UAAY,KACvB,CACLqB,SAAS,EACTpD,MAAO,CACLI,KAAM,qBACNH,QAAS,0DAKR,CACLmD,SAAS,EACTH,KAAM8K,EAEV,GCrfF1E,EAAmBC,iBAAiB6B,GACpC9B,EAAmBC,iBAAiB+E,GACpChF,EAAmBC,iBAAiBqF"}